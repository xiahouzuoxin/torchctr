{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrec_root = '../'\n",
    "\n",
    "import sys\n",
    "sys.path.append(torchrec_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load(f'{torchrec_root}/data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence features from latest to oldest\n",
    "df_samples['his_asin_seq'] = df_samples['his_asin_seq'].map(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'reviewerID', 'dtype': 'category', 'emb_dim': 17, 'min_freq': 3}, {'name': 'asin', 'dtype': 'category', 'emb_dim': 15, 'min_freq': 3}, {'name': 'price', 'dtype': 'numerical', 'norm': 'std', 'mean': 74.40153304932919, 'std': 123.75264929566384}, {'name': 'brand', 'dtype': 'category', 'emb_dim': 11, 'min_freq': 3}, {'name': 'categories', 'dtype': 'category', 'emb_dim': 9, 'min_freq': 3}, {'name': 'his_asin_seq', 'dtype': 'category', 'islist': True, 'emb_dim': 15, 'min_freq': 3, 'max_len': 256}]\n"
     ]
    }
   ],
   "source": [
    "## Hash buckets\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "# ]\n",
    "\n",
    "## Dynamic Embedding\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12},\n",
    "# ]\n",
    "\n",
    "## Auto generate feat_configs\n",
    "from torchrec.utils import auto_generate_feature_configs\n",
    "feat_configs = auto_generate_feature_configs(\n",
    "    df_samples[['reviewerID', 'asin', 'price', 'brand', 'categories', 'his_asin_seq']]\n",
    ")\n",
    "\n",
    "print(feat_configs)\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from torchrec.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchrec.dataset import FeatureTransformer\n",
    "\n",
    "# transformer = FeatureTransformer(feat_configs)\n",
    "\n",
    "# df_train = transformer.transform(df_train, is_train=True, n_jobs=4)\n",
    "# df_test = transformer.transform(df_test, is_train=False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "df_test = pl.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Feature transforming (is_train=True), note that feat_configs will be updated when is_train=True...\n",
      "Input dataFrame type: <class 'polars.dataframe.frame.DataFrame'>, transform it by FeatureTransformerPolars\n",
      "Processing feature reviewerID...\n",
      "Converting category reviewerID to indices...\n",
      "Feature reviewerID vocab size: None -> 153923\n",
      "Processing feature asin...\n",
      "Converting category asin to indices...\n",
      "Feature asin vocab size: None -> 62384\n",
      "Processing feature price...\n",
      "Feature price mean: 74.40153304932919, std: 123.75264929566384, min: 0.01, max: 999.99\n",
      "Processing feature brand...\n",
      "Converting category brand to indices...\n",
      "Feature brand vocab size: None -> 3503\n",
      "Processing feature categories...\n",
      "Converting category categories to indices...\n",
      "Feature categories vocab size: None -> 800\n",
      "Processing feature his_asin_seq...\n",
      "Converting category his_asin_seq to indices...\n",
      "Feature his_asin_seq vocab size: None -> 61925\n",
      "==> Feature transforming (is_train=True) done...\n",
      "==> Dense features: ['price']\n",
      "==> Sparse features: ['reviewerID', 'asin', 'brand', 'categories']\n",
      "==> Sequence dense features: []\n",
      "==> Sequence sparse features: ['his_asin_seq']\n",
      "==> Weight columns mapping: {}\n",
      "==> Target columns: ['label']\n",
      "==> Finished dataset initialization, total samples: 1352538\n"
     ]
    }
   ],
   "source": [
    "from torchrec.dataset import DataFrameDataset\n",
    "\n",
    "train_dataset = DataFrameDataset(df_train, feat_configs, target_cols, is_raw=True, is_train=True, n_jobs=1, verbose=True)\n",
    "\n",
    "feat_configs = train_dataset.transformer.get_feat_configs()\n",
    "test_dataset = DataFrameDataset(df_test, feat_configs, target_cols, is_raw=True, is_train=False, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([v['idx'] for k,v in feat_configs[3]['vocab'].items()])\n",
    "# feat_configs[3]['num_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reviewerID</th><th>asin</th><th>unixReviewTime</th><th>overall</th><th>title</th><th>price</th><th>brand</th><th>categories</th><th>label</th><th>his_asin_seq</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;A1PC34OXBBHDEF&quot;</td><td>&quot;B0000BZL5A&quot;</td><td>1375660800</td><td>4.0</td><td>&quot;B+W 58mm Kaesemann Circular Po…</td><td>89.0</td><td>&quot;B+W&quot;</td><td>&quot;Polarizing Filters&quot;</td><td>1</td><td>[&quot;B0000BZL1P&quot;]</td></tr><tr><td>&quot;A27A0U9O1HUSC8&quot;</td><td>&quot;B007IV7KRU&quot;</td><td>1342137600</td><td>5.0</td><td>&quot;OtterBox Defender Series Case …</td><td>64.97</td><td>&quot;OtterBox&quot;</td><td>&quot;Cases&quot;</td><td>1</td><td>[&quot;B004JMZZE6&quot;, &quot;B005O22Y7G&quot;, &quot;B004RDWVUS&quot;]</td></tr><tr><td>&quot;A1FMND912KUYSX&quot;</td><td>&quot;B006ZW4HY2&quot;</td><td>1397088000</td><td>5.0</td><td>&quot;Olympus VN-702PC Voice Recorde…</td><td>58.98</td><td>&quot;Olympus&quot;</td><td>&quot;Digital Voice Recorders&quot;</td><td>1</td><td>[&quot;B003ANVQWU&quot;, &quot;B000RT77I2&quot;, &quot;B000652M6Y&quot;]</td></tr><tr><td>&quot;A27LEATCCMJJF4&quot;</td><td>&quot;B001D2LJ3Q&quot;</td><td>1357516800</td><td>4.0</td><td>&quot;Manfrotto 701HDV Pro Fluid Vid…</td><td>299.99</td><td>null</td><td>&quot;Tripod Heads&quot;</td><td>1</td><td>[&quot;B005FYNSPK&quot;, &quot;B003D5MZUW&quot;]</td></tr><tr><td>&quot;A1SMJ3J49A59FX&quot;</td><td>&quot;B008DWYBZM&quot;</td><td>1365724800</td><td>4.0</td><td>&quot;Bear Motion Luxury Buffalo Hid…</td><td>49.99</td><td>&quot;Bear Motion&quot;</td><td>&quot;Cases&quot;</td><td>1</td><td>[&quot;B005CLPP84&quot;, &quot;B005U0M9B8&quot;, &quot;B002L6HDTC&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌────────────┬────────────┬────────────┬─────────┬───┬────────────┬────────────┬───────┬───────────┐\n",
       "│ reviewerID ┆ asin       ┆ unixReview ┆ overall ┆ … ┆ brand      ┆ categories ┆ label ┆ his_asin_ │\n",
       "│ ---        ┆ ---        ┆ Time       ┆ ---     ┆   ┆ ---        ┆ ---        ┆ ---   ┆ seq       │\n",
       "│ str        ┆ str        ┆ ---        ┆ f64     ┆   ┆ str        ┆ str        ┆ i64   ┆ ---       │\n",
       "│            ┆            ┆ i64        ┆         ┆   ┆            ┆            ┆       ┆ list[str] │\n",
       "╞════════════╪════════════╪════════════╪═════════╪═══╪════════════╪════════════╪═══════╪═══════════╡\n",
       "│ A1PC34OXBB ┆ B0000BZL5A ┆ 1375660800 ┆ 4.0     ┆ … ┆ B+W        ┆ Polarizing ┆ 1     ┆ [\"B0000BZ │\n",
       "│ HDEF       ┆            ┆            ┆         ┆   ┆            ┆ Filters    ┆       ┆ L1P\"]     │\n",
       "│ A27A0U9O1H ┆ B007IV7KRU ┆ 1342137600 ┆ 5.0     ┆ … ┆ OtterBox   ┆ Cases      ┆ 1     ┆ [\"B004JMZ │\n",
       "│ USC8       ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ ZE6\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ 05O22Y7G\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ , \"…      │\n",
       "│ A1FMND912K ┆ B006ZW4HY2 ┆ 1397088000 ┆ 5.0     ┆ … ┆ Olympus    ┆ Digital    ┆ 1     ┆ [\"B003ANV │\n",
       "│ UYSX       ┆            ┆            ┆         ┆   ┆            ┆ Voice      ┆       ┆ QWU\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆ Recorders  ┆       ┆ 00RT77I2\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ , \"…      │\n",
       "│ A27LEATCCM ┆ B001D2LJ3Q ┆ 1357516800 ┆ 4.0     ┆ … ┆ null       ┆ Tripod     ┆ 1     ┆ [\"B005FYN │\n",
       "│ JJF4       ┆            ┆            ┆         ┆   ┆            ┆ Heads      ┆       ┆ SPK\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ 03D5MZUW\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ ]         │\n",
       "│ A1SMJ3J49A ┆ B008DWYBZM ┆ 1365724800 ┆ 4.0     ┆ … ┆ Bear       ┆ Cases      ┆ 1     ┆ [\"B005CLP │\n",
       "│ 59FX       ┆            ┆            ┆         ┆   ┆ Motion     ┆            ┆       ┆ P84\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ 05U0M9B8\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ , \"…      │\n",
       "└────────────┴────────────┴────────────┴─────────┴───┴────────────┴────────────┴───────┴───────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=2, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=2, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.2781],\n",
      "        [-0.5528]]), 'reviewerID': tensor([[76537],\n",
      "        [78104]], dtype=torch.int32), 'asin': tensor([[19008],\n",
      "        [26286]], dtype=torch.int32), 'brand': tensor([[1961],\n",
      "        [1961]], dtype=torch.int32), 'categories': tensor([[351],\n",
      "        [184]], dtype=torch.int32), 'his_asin_seq': tensor([[42381,  5008],\n",
      "        [21211,  -100]], dtype=torch.int32)}\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset, batch_size=2, num_workers=0, shuffle=True, collate_fn=train_dataset.collate_fn):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Model Input: dense_size=1, sparse_size=67\n",
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(153923, 17)\n",
      "    (asin): Embedding(62384, 15)\n",
      "    (brand): Embedding(3504, 11)\n",
      "    (categories): Embedding(800, 9)\n",
      "    (his_asin_seq): Embedding(61926, 15)\n",
      "  )\n",
      "  (tower): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.002, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:[Validation] Epoch: 0/5, Validation Loss: {'loss': 0.7081270164450614}\n",
      "INFO:Trainer:Learning rate: 0.002\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.7866648435592651}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.4818296018242836}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.43139001533389093}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.4124824810028076}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.4021803920716047}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.39550615698099134}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.39065128356218337}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.3867773515837533}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.3838881490752101}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.3811347994539473}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.3789165760576725}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.37754433938048104}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.3759459740916888}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.37459389035518353}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.37342848594699585}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.372228321492672}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.37120671892538665}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.37032894642914044}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.36940539681249196}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.3686175372255476}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.36781026470661166}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.36698427389065424}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.3661739703064615}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.3655243222998536}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.3647730441515644}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.36418659217357635}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.3636090569312756}\n",
      "INFO:Trainer:[Validation] Epoch: 1/5, Validation Loss: {'loss': 0.34369770607325084}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.002642.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0016\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.34788885712623596}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.3469341412186623}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.3468406906723976}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.3463985581199328}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.3453874296694994}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.3447691478133202}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.3445827594399452}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.3439805548957416}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.34399381186813116}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.3431387667192353}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.34301836770772937}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.34274102121591565}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.3425075514366229}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.3425441660789343}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.3421876013278961}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.34202475267648696}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.3414390169456601}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.3412571950519786}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.3411221508350637}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.3409121274006994}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.3407472971379757}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.3407432903988021}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.3404491427405314}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.34034352367338927}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.34015502319981655}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.3398540068387985}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.3398536220880655}\n",
      "INFO:Trainer:[Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33737593581248926}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.005284.ckpt\n",
      "INFO:Trainer:Learning rate: 0.00128\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.3234991431236267}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.32952803999185565}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.32643117040395736}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.32592894027630487}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.32398735255002975}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.3235951582789421}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.3241634418070316}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.3243180621096066}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.32404824420809747}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.32375862462653054}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.3233643215596676}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.32302260398864746}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.32331464854379494}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.32293733360675664}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32289059074861665}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.3225609826644262}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.32260654332116245}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.32221394541509013}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.32189281249211893}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.32200898244977}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3216849951073527}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.32120310674111047}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.32112275871363555}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.3209498341187187}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.3208263466010491}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.32063602384328843}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.320597689633186}\n",
      "INFO:Trainer:[Validation] Epoch: 3/5, Validation Loss: {'loss': 0.33843121906363133}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0010240000000000002\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.3278820216655731}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.3023051108419895}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.3010002185404301}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.30189639563361803}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.30257617987692353}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.30154136711359025}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.3008701705932617}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.3010743308918817}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.300859067030251}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.3005470517608855}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.29982668463885787}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.3003094506534663}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.300039732158184}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.29972436335224373}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.29963318732167993}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.29954577083388967}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.2993701019976288}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.2993591676038854}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.29903119121988614}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.2989189307470071}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.2986274517700076}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.29848943878497397}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.2984353101456707}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.2980843920487425}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.29800005244091154}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.29801550832986834}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.2979655000567436}\n",
      "INFO:Trainer:[Validation] Epoch: 4/5, Validation Loss: {'loss': 0.349659694681414}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "INFO:Trainer:Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from torchrec.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:Loaded model state_dict from checkpoint.\n",
      "INFO:Trainer:Loaded model.training from checkpoint.\n",
      "INFO:Trainer:Loaded model.feat_configs from checkpoint.\n",
      "INFO:Trainer:Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "INFO:Trainer:Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x31ddc0700> from checkpoint.\n",
      "INFO:Trainer:Loaded logger = <Logger Trainer (INFO)> from checkpoint.\n",
      "INFO:Trainer:Loaded ckpt_file_prefix = checkpoint from checkpoint.\n",
      "INFO:Trainer:Loaded num_epoch = 4 from checkpoint.\n",
      "INFO:Trainer:Loaded global_steps = 10568 from checkpoint.\n",
      "INFO:Trainer:Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "INFO:Trainer:Loaded metadata_fn = ./ckpt//metadata.json from checkpoint.\n",
      "INFO:Trainer:Loaded max_epochs = 5 from checkpoint.\n",
      "INFO:Trainer:Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "INFO:Trainer:Checkpoint loaded from ./ckpt/checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('./ckpt')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.673544966279805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Test service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "curl: (7) Failed to connect to localhost port 8000 after 2 ms: Connection refused\n"
     ]
    }
   ],
   "source": [
    "# Run this on terminal under root of project\n",
    "# !python -m torchrec.serve --name dnn --path examples/ckpt/checkpoint.010568.ckpt --dep_paths examples\n",
    "\n",
    "import os\n",
    "ret = os.system('curl http://localhost:8000/dnn/health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrec.serve import test_predict\n",
    "\n",
    "if ret == 0:\n",
    "    test_predict(df_samples.sample(3), name='dnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
