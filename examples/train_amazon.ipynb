{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrec_root = '../'\n",
    "\n",
    "import sys\n",
    "sys.path.append(torchrec_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load(f'{torchrec_root}/data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence features from latest to oldest\n",
    "df_samples['his_asin_seq'] = df_samples['his_asin_seq'].map(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'reviewerID', 'dtype': 'category', 'emb_dim': 17, 'min_freq': 3}, {'name': 'asin', 'dtype': 'category', 'emb_dim': 15, 'min_freq': 3}, {'name': 'price', 'dtype': 'numerical', 'norm': 'std', 'mean': 74.40153304932919, 'std': 123.75264929566384}, {'name': 'brand', 'dtype': 'category', 'emb_dim': 11, 'min_freq': 3}, {'name': 'categories', 'dtype': 'category', 'emb_dim': 9, 'min_freq': 3}, {'name': 'his_asin_seq', 'dtype': 'category', 'islist': True, 'emb_dim': 15, 'min_freq': 3, 'max_len': 256}]\n"
     ]
    }
   ],
   "source": [
    "## Hash buckets\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "# ]\n",
    "\n",
    "## Dynamic Embedding\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12},\n",
    "# ]\n",
    "\n",
    "## Auto generate feat_configs\n",
    "from torchrec.utils import auto_generate_feature_configs\n",
    "feat_configs = auto_generate_feature_configs(\n",
    "    df_samples[['reviewerID', 'asin', 'price', 'brand', 'categories', 'his_asin_seq']]\n",
    ")\n",
    "\n",
    "print(feat_configs)\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from torchrec.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchrec.dataset import FeatureTransformer\n",
    "\n",
    "# transformer = FeatureTransformer(feat_configs)\n",
    "\n",
    "# df_train = transformer.transform(df_train, is_train=True, n_jobs=4)\n",
    "# df_test = transformer.transform(df_test, is_train=False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Feature transforming (is_train=True), note that feat_configs will be updated when is_train=True...\n",
      "Processing feature reviewerID...\n",
      "Converting category reviewerID to indices...\n",
      "Feature reviewerID vocab size: None -> 153923\n",
      "Processing feature asin...\n",
      "Converting category asin to indices...\n",
      "Feature asin vocab size: None -> 62384\n",
      "Processing feature price...\n",
      "Feature price mean: 74.40153304932919, std: 123.75264929566384, min: 0.01, max: 999.99\n",
      "Processing feature brand...\n",
      "Converting category brand to indices...\n",
      "Feature brand vocab size: None -> 3503\n",
      "Processing feature categories...\n",
      "Converting category categories to indices...\n",
      "Feature categories vocab size: None -> 800\n",
      "Processing feature his_asin_seq...\n",
      "Converting category his_asin_seq to indices...\n",
      "Feature his_asin_seq vocab size: None -> 61925\n",
      "==> Feature transforming (is_train=True) done...\n",
      "==> Dense features: ['price']\n",
      "==> Sparse features: ['reviewerID', 'asin', 'brand', 'categories']\n",
      "==> Sequence dense features: []\n",
      "==> Sequence sparse features: ['his_asin_seq']\n",
      "==> Weight columns mapping: {}\n",
      "==> Target columns: ['label']\n",
      "==> Finished dataset initialization, total samples: 1352538\n"
     ]
    }
   ],
   "source": [
    "from torchrec.dataset import DataFrameDataset\n",
    "\n",
    "train_dataset = DataFrameDataset(df_train, feat_configs, target_cols, is_raw=True, is_train=True, n_jobs=1, verbose=True)\n",
    "test_dataset = DataFrameDataset(df_test, feat_configs, target_cols, is_raw=True, is_train=False, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([v['idx'] for k,v in feat_configs[3]['vocab'].items()])\n",
    "# feat_configs[3]['num_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>title</th>\n",
       "      <th>price</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>label</th>\n",
       "      <th>his_asin_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314370</th>\n",
       "      <td>69777</td>\n",
       "      <td>1870</td>\n",
       "      <td>1375660800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>B+W 58mm Kaesemann Circular Polarizer with Mul...</td>\n",
       "      <td>0.117965</td>\n",
       "      <td>153</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>[102]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538816</th>\n",
       "      <td>47184</td>\n",
       "      <td>140</td>\n",
       "      <td>1342137600</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OtterBox Defender Series Case with Screen Prot...</td>\n",
       "      <td>-0.076213</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[5869, 36203, 387]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194675</th>\n",
       "      <td>111551</td>\n",
       "      <td>2570</td>\n",
       "      <td>1397088000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Olympus VN-702PC Voice Recorder</td>\n",
       "      <td>-0.124616</td>\n",
       "      <td>54</td>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>[18766, 12848, 139]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543055</th>\n",
       "      <td>10936</td>\n",
       "      <td>6521</td>\n",
       "      <td>1357516800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Manfrotto 701HDV Pro Fluid Video Mini Head</td>\n",
       "      <td>1.822898</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>[26, 10157]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355447</th>\n",
       "      <td>99991</td>\n",
       "      <td>3273</td>\n",
       "      <td>1365724800</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bear Motion Luxury Buffalo Hide Vintage Leathe...</td>\n",
       "      <td>-0.197261</td>\n",
       "      <td>550</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[30, 918, 9748]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        reviewerID  asin  unixReviewTime  overall  \\\n",
       "314370       69777  1870      1375660800      4.0   \n",
       "538816       47184   140      1342137600      5.0   \n",
       "194675      111551  2570      1397088000      5.0   \n",
       "543055       10936  6521      1357516800      4.0   \n",
       "355447       99991  3273      1365724800      4.0   \n",
       "\n",
       "                                                    title     price  brand  \\\n",
       "314370  B+W 58mm Kaesemann Circular Polarizer with Mul...  0.117965    153   \n",
       "538816  OtterBox Defender Series Case with Screen Prot... -0.076213     78   \n",
       "194675                    Olympus VN-702PC Voice Recorder -0.124616     54   \n",
       "543055         Manfrotto 701HDV Pro Fluid Video Mini Head  1.822898      1   \n",
       "355447  Bear Motion Luxury Buffalo Hide Vintage Leathe... -0.197261    550   \n",
       "\n",
       "        categories  label         his_asin_seq  \n",
       "314370         197      1                [102]  \n",
       "538816           1      1   [5869, 36203, 387]  \n",
       "194675         140      1  [18766, 12848, 139]  \n",
       "543055         153      1          [26, 10157]  \n",
       "355447           1      1      [30, 918, 9748]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=2, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=2, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.4498],\n",
      "        [ 1.0068]]), 'reviewerID': tensor([[  241],\n",
      "        [15931]], dtype=torch.int32), 'asin': tensor([[6332],\n",
      "        [  10]], dtype=torch.int32), 'brand': tensor([[1],\n",
      "        [1]], dtype=torch.int32), 'categories': tensor([[ 41],\n",
      "        [134]], dtype=torch.int32), 'his_asin_seq': tensor([[  455, 11740,  4495,   778, 10291,  3358,  3952,  1271,  1527, 12399,\n",
      "         20048, 13624, 13349, 17988],\n",
      "        [ 4977,   786,   101,    62,   131,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100]], dtype=torch.int32)}\n",
      "tensor([[1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset, batch_size=2, num_workers=0, shuffle=True, collate_fn=train_dataset.collate_fn):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Model Input: dense_size=1, sparse_size=67\n",
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(153923, 17)\n",
      "    (asin): Embedding(62384, 15)\n",
      "    (brand): Embedding(3504, 11)\n",
      "    (categories): Embedding(800, 9)\n",
      "    (his_asin_seq): Embedding(61926, 15)\n",
      "  )\n",
      "  (tower): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.002, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:[Validation] Epoch: 0/5, Validation Loss: {'loss': 0.7880084404104749}\n",
      "INFO:Trainer:Learning rate: 0.002\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.8044982552528381}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.49722136348485946}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.43981830090284346}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.41882002919912337}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.4077405021339655}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.40028194892406466}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.39551687096556026}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.3911837835822787}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.3878447149693966}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.38530445118745166}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.3835734532773495}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.38169184291904623}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.3805318695058425}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.37891894365732487}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.3776332456299237}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.3759947204192479}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.3745926612243056}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.37361577426686005}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.37257300328877235}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.3714627279262794}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.370507663205266}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.3695130655311403}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.3684536426988515}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.36772345716538635}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.3666963554918766}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.3658910727262497}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.36527171060442926}\n",
      "INFO:Trainer:[Validation] Epoch: 1/5, Validation Loss: {'loss': 0.34370235487320505}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.002642.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0016\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.34476378560066223}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.35442944228649137}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.34855711355805397}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.3467109348376592}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.34589963786303995}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.34500432473421094}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.34534196789065996}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.34476373178618297}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.344406402297318}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.3447514111465878}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.3440020573437214}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.3436693627996878}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.34300301884611445}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.34228769859442343}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.342178032632385}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.34217934743563333}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.3419114034995437}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.3415275423141087}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.3414978589779801}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.3413155105553175}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.34111287514865396}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.3409535694406146}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.34088268439878117}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.34053192053152165}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.34034431202958026}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.34000177401304243}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.3397844898356841}\n",
      "INFO:Trainer:[Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33570210739834333}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.005284.ckpt\n",
      "INFO:Trainer:Learning rate: 0.00128\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.3163418471813202}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.328230162858963}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.3280913862586021}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.328090840280056}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.3277438924461603}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.3265879100263119}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.3259145697206259}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.32546809845737046}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.3244949249736965}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.32375604174203343}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.32349594359099865}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.32346966327591375}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.32318389675269527}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.32313742542496093}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32287757793707506}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.32235945005218186}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.32217302500270306}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.3220117112818886}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.3218570624374681}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.3217322342097759}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3215744699984789}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.32123353798474585}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.32098245794800195}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.32086909121145374}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.32090575750296313}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.32083150966763496}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.32078790613091906}\n",
      "INFO:Trainer:[Validation] Epoch: 3/5, Validation Loss: {'loss': 0.33414375963182075}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0010240000000000002\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.304471880197525}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.3057867436110973}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.30224318377673626}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.3015756830573082}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.3009458491206169}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.30068583714962005}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.3005599697927634}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.3001842957522188}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.30051658492535355}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.29999443916810886}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.2997591637223959}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.29996620069850577}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.2995374914507071}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.2991486085263582}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.2988655558547803}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.2985576773583889}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.29876875528134406}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.29886055657092264}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.2985641808476713}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.2988206883480674}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.298585806876421}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.2984303938987709}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.29817004801197483}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.29819388820425324}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.2980360375034312}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.2979001172840595}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.2976831844334419}\n",
      "INFO:Trainer:[Validation] Epoch: 4/5, Validation Loss: {'loss': 0.33929263666770376}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "INFO:Trainer:Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from torchrec.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:Loaded model state_dict from checkpoint.\n",
      "INFO:Trainer:Loaded model.training from checkpoint.\n",
      "INFO:Trainer:Loaded model.feat_configs from checkpoint.\n",
      "INFO:Trainer:Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "INFO:Trainer:Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x2f801e710> from checkpoint.\n",
      "INFO:Trainer:Loaded logger = <Logger Trainer (INFO)> from checkpoint.\n",
      "INFO:Trainer:Loaded ckpt_file_prefix = checkpoint from checkpoint.\n",
      "INFO:Trainer:Loaded num_epoch = 4 from checkpoint.\n",
      "INFO:Trainer:Loaded global_steps = 10568 from checkpoint.\n",
      "INFO:Trainer:Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "INFO:Trainer:Loaded metadata_fn = ./ckpt//metadata.json from checkpoint.\n",
      "INFO:Trainer:Loaded max_epochs = 5 from checkpoint.\n",
      "INFO:Trainer:Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "INFO:Trainer:Checkpoint loaded from ./ckpt/checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('./ckpt')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6787381252663678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Test service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"ok\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   3966      0 --:--:-- --:--:-- --:--:-- 15000\n"
     ]
    }
   ],
   "source": [
    "# Run this on terminal under root of project\n",
    "# !python -m torchrec.serve --name dnn --path examples/ckpt/checkpoint.010568.ckpt --dep_paths examples\n",
    "\n",
    "import os\n",
    "ret = os.system('curl http://localhost:8000/dnn/health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'features': {'reviewerID': ['A2A0XVXWTZGW1I', 'A32TJ28E634ZMQ', 'A2WKNMP58J3RYZ'], 'asin': ['B0042SGDVG', 'B003D5MY5I', 'B005H21LPO'], 'unixReviewTime': [1333929600, 1323216000, 1354060800], 'overall': [5.0, 3.0, 4.0], 'title': ['Samsung Spinpoint F4EG 2 TB SATA2 5400rpm 32 MB Hard Drive HD204UI/Z4', 'SanDisk Extreme HD Video 16 GB SDHC Class 10 Memory Card (SDSDRX3-016G-A21)', 'Fenzer Rechargeable Cordless Phone Battery for AT&amp;T/Lucent BT-18433 BT-184342 Cordless Telephone Battery Replacement Pack'], 'price': [134.99, 16.0, 2.58], 'brand': [nan, 'SanDisk', 'Fenzer'], 'categories': ['Internal Hard Drives', 'SD & SDHC Cards', 'Batteries'], 'label': [1, 1, 1], 'his_asin_seq': [['B002SZEOLG', 'B002BH3Z8E', 'B001O4EPHA', 'B001E1Y5O6', 'B000HPV3RW', 'B002RJUSO0', 'B000BFLFHQ'], ['B001KB6Z2U'], ['B000FUVNX8', 'B002Y1U2M4', 'B002ZCXJZE', 'B000BF106C', 'B0009K9FZW', 'B000089GN2']]}}\n",
      "Prediction: {'prediction': [[5.196547031402588], [1.7335100173950195], [3.3409905433654785]]}\n"
     ]
    }
   ],
   "source": [
    "from torchrec.serve import test_predict\n",
    "\n",
    "if ret == 0:\n",
    "    test_predict(df_samples.sample(3), name='dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
