{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchrec_root = '../'\n",
    "\n",
    "import sys\n",
    "sys.path.append(torchrec_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load(f'{torchrec_root}/data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence features from latest to oldest\n",
    "df_samples['his_asin_seq'] = df_samples['his_asin_seq'].map(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'reviewerID', 'dtype': 'category', 'emb_dim': 17, 'min_freq': 3}, {'name': 'asin', 'dtype': 'category', 'emb_dim': 15, 'min_freq': 3}, {'name': 'price', 'dtype': 'numerical', 'norm': 'std', 'mean': np.float64(74.40153304932919), 'std': np.float64(123.75264929565961)}, {'name': 'brand', 'dtype': 'category', 'emb_dim': 11, 'min_freq': 3}, {'name': 'categories', 'dtype': 'category', 'emb_dim': 9, 'min_freq': 3}, {'name': 'his_asin_seq', 'dtype': 'category', 'islist': True, 'emb_dim': 15, 'min_freq': 3, 'max_len': 256}]\n"
     ]
    }
   ],
   "source": [
    "## Hash buckets\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "# ]\n",
    "\n",
    "## Dynamic Embedding\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12},\n",
    "# ]\n",
    "\n",
    "## Auto generate feat_configs\n",
    "from torchrec.utils import auto_generate_feature_configs\n",
    "feat_configs = auto_generate_feature_configs(\n",
    "    df_samples[['reviewerID', 'asin', 'price', 'brand', 'categories', 'his_asin_seq']]\n",
    ")\n",
    "\n",
    "print(feat_configs)\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from torchrec.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchrec.dataset import FeatureTransformer\n",
    "\n",
    "# transformer = FeatureTransformer(feat_configs)\n",
    "\n",
    "# df_train = transformer.transform(df_train, is_train=True, n_jobs=4)\n",
    "# df_test = transformer.transform(df_test, is_train=False, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "df_test = pl.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Feature transforming (is_train=True), note that feat_configs will be updated when is_train=True...\n",
      "Input dataFrame type: <class 'polars.dataframe.frame.DataFrame'>, transform it by FeatureTransformerPolars\n",
      "Processing feature reviewerID...\n",
      "Converting category reviewerID to indices...\n",
      "Feature reviewerID vocab size: None -> 153923\n",
      "Processing feature asin...\n",
      "Converting category asin to indices...\n",
      "Feature asin vocab size: None -> 62384\n",
      "Processing feature price...\n",
      "Feature price mean: 74.40153304932919, std: 123.75264929565961, min: 0.01, max: 999.99\n",
      "Processing feature brand...\n",
      "Converting category brand to indices...\n",
      "Feature brand vocab size: None -> 3503\n",
      "Processing feature categories...\n",
      "Converting category categories to indices...\n",
      "Feature categories vocab size: None -> 800\n",
      "Processing feature his_asin_seq...\n",
      "Converting category his_asin_seq to indices...\n",
      "Feature his_asin_seq vocab size: None -> 61925\n",
      "==> Feature transforming (is_train=True) done...\n",
      "==> Dense features: ['price']\n",
      "==> Sparse features: ['reviewerID', 'asin', 'brand', 'categories']\n",
      "==> Sequence dense features: []\n",
      "==> Sequence sparse features: ['his_asin_seq']\n",
      "==> Weight columns mapping: {}\n",
      "==> Target columns: ['label']\n",
      "==> Finished dataset initialization, total samples: 1352538\n"
     ]
    }
   ],
   "source": [
    "from torchrec.dataset import DataFrameDataset\n",
    "\n",
    "train_dataset = DataFrameDataset(df_train, feat_configs, target_cols, is_raw=True, is_train=True, n_jobs=1, verbose=True)\n",
    "\n",
    "feat_configs = train_dataset.transformer.get_feat_configs()\n",
    "test_dataset = DataFrameDataset(df_test, feat_configs, target_cols, is_raw=True, is_train=False, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max([v['idx'] for k,v in feat_configs[3]['vocab'].items()])\n",
    "# feat_configs[3]['num_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reviewerID</th><th>asin</th><th>unixReviewTime</th><th>overall</th><th>title</th><th>price</th><th>brand</th><th>categories</th><th>label</th><th>his_asin_seq</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>i64</td><td>list[str]</td></tr></thead><tbody><tr><td>&quot;A1PC34OXBBHDEF&quot;</td><td>&quot;B0000BZL5A&quot;</td><td>1375660800</td><td>4.0</td><td>&quot;B+W 58mm Kaesemann Circular Po…</td><td>89.0</td><td>&quot;B+W&quot;</td><td>&quot;Polarizing Filters&quot;</td><td>1</td><td>[&quot;B0000BZL1P&quot;]</td></tr><tr><td>&quot;A27A0U9O1HUSC8&quot;</td><td>&quot;B007IV7KRU&quot;</td><td>1342137600</td><td>5.0</td><td>&quot;OtterBox Defender Series Case …</td><td>64.97</td><td>&quot;OtterBox&quot;</td><td>&quot;Cases&quot;</td><td>1</td><td>[&quot;B004JMZZE6&quot;, &quot;B005O22Y7G&quot;, &quot;B004RDWVUS&quot;]</td></tr><tr><td>&quot;A1FMND912KUYSX&quot;</td><td>&quot;B006ZW4HY2&quot;</td><td>1397088000</td><td>5.0</td><td>&quot;Olympus VN-702PC Voice Recorde…</td><td>58.98</td><td>&quot;Olympus&quot;</td><td>&quot;Digital Voice Recorders&quot;</td><td>1</td><td>[&quot;B003ANVQWU&quot;, &quot;B000RT77I2&quot;, &quot;B000652M6Y&quot;]</td></tr><tr><td>&quot;A27LEATCCMJJF4&quot;</td><td>&quot;B001D2LJ3Q&quot;</td><td>1357516800</td><td>4.0</td><td>&quot;Manfrotto 701HDV Pro Fluid Vid…</td><td>299.99</td><td>null</td><td>&quot;Tripod Heads&quot;</td><td>1</td><td>[&quot;B005FYNSPK&quot;, &quot;B003D5MZUW&quot;]</td></tr><tr><td>&quot;A1SMJ3J49A59FX&quot;</td><td>&quot;B008DWYBZM&quot;</td><td>1365724800</td><td>4.0</td><td>&quot;Bear Motion Luxury Buffalo Hid…</td><td>49.99</td><td>&quot;Bear Motion&quot;</td><td>&quot;Cases&quot;</td><td>1</td><td>[&quot;B005CLPP84&quot;, &quot;B005U0M9B8&quot;, &quot;B002L6HDTC&quot;]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌────────────┬────────────┬────────────┬─────────┬───┬────────────┬────────────┬───────┬───────────┐\n",
       "│ reviewerID ┆ asin       ┆ unixReview ┆ overall ┆ … ┆ brand      ┆ categories ┆ label ┆ his_asin_ │\n",
       "│ ---        ┆ ---        ┆ Time       ┆ ---     ┆   ┆ ---        ┆ ---        ┆ ---   ┆ seq       │\n",
       "│ str        ┆ str        ┆ ---        ┆ f64     ┆   ┆ str        ┆ str        ┆ i64   ┆ ---       │\n",
       "│            ┆            ┆ i64        ┆         ┆   ┆            ┆            ┆       ┆ list[str] │\n",
       "╞════════════╪════════════╪════════════╪═════════╪═══╪════════════╪════════════╪═══════╪═══════════╡\n",
       "│ A1PC34OXBB ┆ B0000BZL5A ┆ 1375660800 ┆ 4.0     ┆ … ┆ B+W        ┆ Polarizing ┆ 1     ┆ [\"B0000BZ │\n",
       "│ HDEF       ┆            ┆            ┆         ┆   ┆            ┆ Filters    ┆       ┆ L1P\"]     │\n",
       "│ A27A0U9O1H ┆ B007IV7KRU ┆ 1342137600 ┆ 5.0     ┆ … ┆ OtterBox   ┆ Cases      ┆ 1     ┆ [\"B004JMZ │\n",
       "│ USC8       ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ ZE6\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ 05O22Y7G\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ , \"…      │\n",
       "│ A1FMND912K ┆ B006ZW4HY2 ┆ 1397088000 ┆ 5.0     ┆ … ┆ Olympus    ┆ Digital    ┆ 1     ┆ [\"B003ANV │\n",
       "│ UYSX       ┆            ┆            ┆         ┆   ┆            ┆ Voice      ┆       ┆ QWU\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆ Recorders  ┆       ┆ 00RT77I2\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ , \"…      │\n",
       "│ A27LEATCCM ┆ B001D2LJ3Q ┆ 1357516800 ┆ 4.0     ┆ … ┆ null       ┆ Tripod     ┆ 1     ┆ [\"B005FYN │\n",
       "│ JJF4       ┆            ┆            ┆         ┆   ┆            ┆ Heads      ┆       ┆ SPK\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ 03D5MZUW\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ ]         │\n",
       "│ A1SMJ3J49A ┆ B008DWYBZM ┆ 1365724800 ┆ 4.0     ┆ … ┆ Bear       ┆ Cases      ┆ 1     ┆ [\"B005CLP │\n",
       "│ 59FX       ┆            ┆            ┆         ┆   ┆ Motion     ┆            ┆       ┆ P84\", \"B0 │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ 05U0M9B8\" │\n",
       "│            ┆            ┆            ┆         ┆   ┆            ┆            ┆       ┆ , \"…      │\n",
       "└────────────┴────────────┴────────────┴─────────┴───┴────────────┴────────────┴───────┴───────────┘"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=512, num_workers=2, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, num_workers=2, shuffle=False, collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.3660],\n",
      "        [ 0.0000]]), 'reviewerID': tensor([[ 69895],\n",
      "        [150251]], dtype=torch.int32), 'asin': tensor([[19411],\n",
      "        [ 3263]], dtype=torch.int32), 'brand': tensor([[3069],\n",
      "        [1631]], dtype=torch.int32), 'categories': tensor([[356],\n",
      "        [ 98]], dtype=torch.int32), 'his_asin_seq': tensor([[45384,  -100,  -100],\n",
      "        [37069, 24828,  9744]], dtype=torch.int32)}\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset, batch_size=2, num_workers=0, shuffle=True, collate_fn=train_dataset.collate_fn):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Model Input: dense_size=1, sparse_size=67\n",
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(153923, 17)\n",
      "    (asin): Embedding(62384, 15)\n",
      "    (brand): Embedding(3504, 11)\n",
      "    (categories): Embedding(800, 9)\n",
      "    (his_asin_seq): Embedding(61926, 15)\n",
      "  )\n",
      "  (tower): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchrec.models import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.002, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Trainer:[Validation] Epoch: 0/5, Validation Loss: {'loss': 0.7358333799976708}\n",
      "INFO:Trainer:Learning rate: 0.002\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.7672242522239685}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.4826859492063522}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.4303086479008198}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.4131175540884336}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.40101835161447524}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.3949488976597786}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.39040266101559}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.38753292139087403}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.3848135073855519}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.38245977368619705}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.380652304649353}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.37930671128359705}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.37770890245834987}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.37594352178848706}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.3746101864320891}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.37334601535399753}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.37214342864230276}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.37122757587362737}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.37014431700110434}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.3692953340944491}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.36824311736226084}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.36736465445586614}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.3664894697747447}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.3656783656063287}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.3649993961552779}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.36418967612981795}\n",
      "INFO:Trainer:[Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.36362986429379535}\n",
      "INFO:Trainer:[Validation] Epoch: 1/5, Validation Loss: {'loss': 0.34373576078552603}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.002642.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0016\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.36313849687576294}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.3502950271964073}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.348273516446352}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.34706757922967274}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.34702457979321477}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.346574978351593}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.34703235735495885}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.3456289068715913}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.3453291705995798}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.3447235346833865}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.34473623552918436}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.3449023152481426}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.3442147838324308}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.3438208688680942}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.34363861548049107}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.34348835839827857}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.3429463760368526}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.343075713725651}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.34273195296525955}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.34275419346596064}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.3424577472805977}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.3421420098344485}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.341557599184188}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.34135955147121266}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.34118248300005993}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.3408270393371582}\n",
      "INFO:Trainer:[Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.3406127237356626}\n",
      "INFO:Trainer:[Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33663016897385606}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.005284.ckpt\n",
      "INFO:Trainer:Learning rate: 0.00128\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.3316836953163147}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.3334489420056343}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.3307785102725029}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.32971966723601026}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.3290090143308044}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.32794582614302636}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.3272024734069904}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.32613333091139796}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.326349676605314}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.32583808450235263}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.3249753309637308}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.3245998957346786}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.3244349682206909}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.32434347334962627}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32416855142584866}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.3240652827322483}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.32424006088636814}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.3240103915070786}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.3238808569891585}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.32372591071222956}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3235930204093456}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.3234640618874913}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.3233219770883972}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.32288452729582784}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.32279083005463083}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.3225255884349346}\n",
      "INFO:Trainer:[Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.32246563196755373}\n",
      "INFO:Trainer:[Validation] Epoch: 3/5, Validation Loss: {'loss': 0.33648246585598107}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "INFO:Trainer:Learning rate: 0.0010240000000000002\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.29300427436828613}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.3051088374853134}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.30363920219242574}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.30291040653983753}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.3021452172100544}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.3009887849390507}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.30017747543752193}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.3000922640519483}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.29986280292272566}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.2997000841299693}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.299868129581213}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.2999936965920708}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.29975710074106854}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.3000094058536566}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.2999421140232256}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.299749033143123}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.29977126359008255}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.29949417865451644}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.29966579732795556}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.29959166663257697}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.29950623072683813}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.29932588597848303}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.29934715850109406}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.2994616183055484}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.2995197870892783}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.2994160011947155}\n",
      "INFO:Trainer:[Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.29948702197808486}\n",
      "INFO:Trainer:[Validation] Epoch: 4/5, Validation Loss: {'loss': 0.3436457416478624}\n",
      "INFO:Trainer:Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "INFO:Trainer:Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from torchrec.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wsl/dev/torchrec/examples/../torchrec/trainer.py:278: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_file)\n",
      "INFO:Trainer:Loaded model state_dict from checkpoint.\n",
      "INFO:Trainer:Loaded model.training from checkpoint.\n",
      "INFO:Trainer:Loaded model.feat_configs from checkpoint.\n",
      "INFO:Trainer:Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "INFO:Trainer:Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x7fe3c5e1d8e0> from checkpoint.\n",
      "INFO:Trainer:Loaded logger = <Logger Trainer (INFO)> from checkpoint.\n",
      "INFO:Trainer:Loaded ckpt_file_prefix = checkpoint from checkpoint.\n",
      "INFO:Trainer:Loaded num_epoch = 4 from checkpoint.\n",
      "INFO:Trainer:Loaded global_steps = 10568 from checkpoint.\n",
      "INFO:Trainer:Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "INFO:Trainer:Loaded metadata_fn = ./ckpt//metadata.json from checkpoint.\n",
      "INFO:Trainer:Loaded max_epochs = 5 from checkpoint.\n",
      "INFO:Trainer:Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "INFO:Trainer:Checkpoint loaded from ./ckpt/checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('./ckpt')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6766264784924219\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Test service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"ok\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   4004      0 --:--:-- --:--:-- --:--:--  5000\n"
     ]
    }
   ],
   "source": [
    "# Run this on terminal under root of project\n",
    "# !python -m torchrec.serve --name dnn --path examples/ckpt/checkpoint.010568.ckpt --dep_paths examples\n",
    "\n",
    "import os\n",
    "ret = os.system('curl http://localhost:8000/dnn/health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(df, name):\n",
    "    ''' \n",
    "    Test the prediction of the model.\n",
    "    First launch the server by `python -m torchrec.serve --name {name} --path path/to/model`\n",
    "    '''\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    data = {\n",
    "        'features': df.to_dict(orient='list')\n",
    "    }\n",
    "    print(f\"Data: {data}\")\n",
    "    data_json = json.dumps(data)\n",
    "    response = requests.post(\n",
    "        f\"http://localhost:8000/{name}/predict\", \n",
    "        data=data_json, \n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'features': {'reviewerID': ['A2OIX8THCUGT1H', 'A2V1J3JT5OOZFO', 'A3I9U7LFVD3OYH'], 'asin': ['B000WL6YY8', 'B0038W0K2K', 'B00GPUNJ02'], 'unixReviewTime': [1386547200, 1348704000, 1391472000], 'overall': [4.0, 5.0, 5.0], 'title': ['Cheetah Mounts Plasma LCD Flat Screen TV Articulating Full Motion Dual Arm Wall Mount Bracket For 32-65&quot; Displays Up To 165LBS Black With 10 Foot 1.4 HDMI Cable Fits Up To 24&quot; Studs', 'MEElectronics Sport-Fi M6 Noise-Isolating In-Ear Headphones with Memory Wire (Black)', \"iBenzer - Laptop black Sleeve Bag Cover Case for Macbook Pro 13'' with retina display A1425 &amp; A1502 (Macbook Pro 13 Retina, Black)\"], 'price': [80.85, 18.99, 8.99], 'brand': ['Cheetah', 'MEElectronics', nan], 'categories': ['TV Ceiling & Wall Mounts', 'Headphones', 'Sleeves & Slipcases'], 'label': [1, 1, 1], 'his_asin_seq': [['B00CDIK908', 'B0052GVNCQ', 'B003UCGDSS', 'B00005LD4T'], ['B003I4FHNA', 'B001F42MKG', 'B0052D38UO', 'B005S6XUXA', 'B0064S7YQU', 'B002FU6KF2', 'B002E4UWWQ', 'B000WALWW8', 'B000Q6WEFW', 'B000A8DSY2', 'B004K1EZDS', 'B002LPV6LY', 'B0002F3G7M', 'B0046A8Y8A', 'B003OGNXZQ', 'B003K0ZPN4', 'B00354HS3A', 'B002GWJ8N0', 'B001B077PO', 'B0000AQNX2', 'B001H9O73O', 'B00009W2GI', 'B001FWBUUG', 'B0018MJ9ZG', 'B0002J28N6', 'B0002CWPS6', 'B001MPWMDA', 'B000P6M6HY', 'B001L5SN70', 'B0002KR74A', 'B0002EOFFK', 'B000WOIFO2', 'B0017TYGMG', 'B0013L2FEE', 'B0011Z2402', 'B0001FTVEK'], ['B00EXV66R8', 'B00BN0MZY0', 'B0076R7F62', 'B000F3VHV8', 'B003MVZ60I', 'B000SAB34O', 'B000NZX2CA']]}}\n",
      "Prediction: {'prediction': [[2.088313341140747], [1.1368013620376587], [1.0401537418365479]]}\n"
     ]
    }
   ],
   "source": [
    "if ret == 0:\n",
    "    test_predict(df_samples.sample(3), name='dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
