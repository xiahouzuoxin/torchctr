{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/monkeyzx/miniconda3/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchctr_root = '../'\n",
    "\n",
    "import sys\n",
    "sys.path.append(torchctr_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load(f'{torchctr_root}/data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence features from latest to oldest\n",
    "df_samples['his_asin_seq'] = df_samples['his_asin_seq'].map(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'reviewerID', 'dtype': 'category', 'emb_dim': 17, 'min_freq': 3}, {'name': 'asin', 'dtype': 'category', 'emb_dim': 15, 'min_freq': 3}, {'name': 'price', 'dtype': 'numerical', 'norm': 'std', 'mean': np.float64(74.40153304932919), 'std': np.float64(123.75264929565961)}, {'name': 'brand', 'dtype': 'category', 'emb_dim': 11, 'min_freq': 3}, {'name': 'categories', 'dtype': 'category', 'emb_dim': 9, 'min_freq': 3}, {'name': 'his_asin_seq', 'dtype': 'category', 'islist': True, 'emb_dim': 15, 'min_freq': 3, 'max_len': 256}]\n"
     ]
    }
   ],
   "source": [
    "## Hash buckets\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3, \"hash_buckets\": 1000000},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12, \"hash_buckets\": 1000000},\n",
    "# ]\n",
    "\n",
    "## Dynamic Embedding\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 12, \"min_freq\": 3},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": \"std\"},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\", \"min_freq\": 3, \"emb_dim\": 12},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"islist\": True, \"min_freq\": 3, \"emb_dim\": 12},\n",
    "# ]\n",
    "\n",
    "## Auto generate feat_configs\n",
    "from torchctr.utils import auto_generate_feature_configs\n",
    "feat_configs = auto_generate_feature_configs(\n",
    "    df_samples[['reviewerID', 'asin', 'price', 'brand', 'categories', 'his_asin_seq']]\n",
    ")\n",
    "\n",
    "print(feat_configs)\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352538 336650\n"
     ]
    }
   ],
   "source": [
    "from torchctr.sample import traintest_split\n",
    "\n",
    "df_train, df_test = traintest_split(df_samples, test_size=0.2, shuffle=True, group_id='reviewerID')\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchctr.dataset import FeatureTransformer\n",
    "\n",
    "# transformer = FeatureTransformer(feat_configs, category_dynamic_vocab=True, category_min_freq=3, verbose=True)\n",
    "# transformer.fit(df_train)\n",
    "# df_train = transformer.transform(df_train)\n",
    "# df_test = transformer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pl.DataFrame\n",
    "import polars as pl\n",
    "\n",
    "df_train = pl.from_pandas(df_train)\n",
    "df_test = pl.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 21:40:09 torchctr INFO - Feature transforming (is_train=True), note that feat_configs will be updated when is_train=True...\n",
      "2024-11-11 21:40:09 torchctr INFO - Input dataFrame type: <class 'polars.dataframe.frame.DataFrame'>, transform it by FeatureTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 21:40:14 torchctr INFO - Processing feature reviewerID...\n",
      "2024-11-11 21:40:14 torchctr INFO - Processing feature price...\n",
      "2024-11-11 21:40:14 torchctr INFO - Feature price mean: 74.40153304932919, std: 123.75264929565961, min: 0.01, max: 999.99\n",
      "2024-11-11 21:40:15 torchctr INFO - Processing feature asin...\n",
      "2024-11-11 21:40:15 torchctr INFO - Processing feature categories...\n",
      "2024-11-11 21:40:15 torchctr INFO - Processing feature brand...\n",
      "2024-11-11 21:40:16 torchctr INFO - Converting category reviewerID to indices...\n",
      "2024-11-11 21:40:16 torchctr INFO - Converting category asin to indices...\n",
      "2024-11-11 21:40:16 torchctr INFO - Converting category brand to indices...\n",
      "2024-11-11 21:40:16 torchctr INFO - Feature brand vocab size: None -> 3503\n",
      "2024-11-11 21:40:16 torchctr INFO - Feature asin vocab size: None -> 62384\n",
      "2024-11-11 21:40:16 torchctr INFO - Feature reviewerID vocab size: None -> 153923\n",
      "2024-11-11 21:40:16 torchctr INFO - Converting category categories to indices...\n",
      "2024-11-11 21:40:16 torchctr INFO - Feature categories vocab size: None -> 800\n",
      "2024-11-11 21:40:18 torchctr INFO - Processing feature his_asin_seq...\n",
      "2024-11-11 21:40:26 torchctr INFO - Converting category his_asin_seq to indices...\n",
      "2024-11-11 21:40:26 torchctr INFO - Feature his_asin_seq vocab size: None -> 61925\n",
      "2024-11-11 21:40:38 torchctr INFO - Feature transforming (is_train=False), note that feat_configs will be updated when is_train=True...\n",
      "2024-11-11 21:40:38 torchctr INFO - Input dataFrame type: <class 'polars.dataframe.frame.DataFrame'>, transform it by FeatureTransformer\n",
      "2024-11-11 21:40:39 torchctr INFO - Processing feature reviewerID...\n",
      "2024-11-11 21:40:39 torchctr INFO - Converting category reviewerID to indices...\n",
      "2024-11-11 21:40:39 torchctr INFO - Processing feature asin...\n",
      "2024-11-11 21:40:39 torchctr INFO - Converting category asin to indices...\n",
      "2024-11-11 21:40:39 torchctr INFO - Processing feature price...\n",
      "2024-11-11 21:40:40 torchctr INFO - Processing feature categories...\n",
      "2024-11-11 21:40:40 torchctr INFO - Converting category categories to indices...\n",
      "2024-11-11 21:40:41 torchctr INFO - Processing feature his_asin_seq...\n",
      "2024-11-11 21:40:42 torchctr INFO - Converting category his_asin_seq to indices...\n",
      "2024-11-11 21:40:43 torchctr INFO - Processing feature brand...\n",
      "2024-11-11 21:40:43 torchctr INFO - Converting category brand to indices...\n"
     ]
    }
   ],
   "source": [
    "from torchctr.dataset import FeatureTransformer\n",
    "\n",
    "transformer = FeatureTransformer(feat_configs, category_dynamic_vocab=True, category_min_freq=3, verbose=True)\n",
    "\n",
    "df_train = transformer.fit_transform(df_train, is_train=True, n_jobs=4)\n",
    "df_test = transformer.transform(df_test, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reviewerID</th><th>asin</th><th>unixReviewTime</th><th>overall</th><th>title</th><th>price</th><th>brand</th><th>categories</th><th>label</th><th>his_asin_seq</th></tr><tr><td>i32</td><td>i32</td><td>i64</td><td>f64</td><td>str</td><td>f64</td><td>i32</td><td>i32</td><td>i64</td><td>list[i32]</td></tr></thead><tbody><tr><td>0</td><td>60021</td><td>1386547200</td><td>4.0</td><td>&quot;Mediabridge 16AWG Speaker Wire…</td><td>-0.480083</td><td>217</td><td>568</td><td>1</td><td>[41675, 5429]</td></tr><tr><td>0</td><td>61124</td><td>1362268800</td><td>5.0</td><td>&quot;Monoprice 107165 3.5mm Stereo …</td><td>-0.583757</td><td>1929</td><td>459</td><td>1</td><td>[59789]</td></tr><tr><td>0</td><td>56065</td><td>1355270400</td><td>5.0</td><td>&quot;Poetic (TM) PU Folio Case for …</td><td>-0.601131</td><td>1223</td><td>576</td><td>1</td><td>[6154]</td></tr><tr><td>0</td><td>59007</td><td>1210204800</td><td>5.0</td><td>&quot;Sony STRDG720 7.1 Audio Video …</td><td>1.984188</td><td>1223</td><td>607</td><td>1</td><td>[4982, 61032, … 15221]</td></tr><tr><td>0</td><td>56272</td><td>1387670400</td><td>5.0</td><td>&quot;Corsair Vengeance  16GB (2x8GB…</td><td>0.891847</td><td>750</td><td>425</td><td>1</td><td>[43254]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌────────────┬───────┬────────────────┬─────────┬───┬───────┬────────────┬───────┬─────────────────┐\n",
       "│ reviewerID ┆ asin  ┆ unixReviewTime ┆ overall ┆ … ┆ brand ┆ categories ┆ label ┆ his_asin_seq    │\n",
       "│ ---        ┆ ---   ┆ ---            ┆ ---     ┆   ┆ ---   ┆ ---        ┆ ---   ┆ ---             │\n",
       "│ i32        ┆ i32   ┆ i64            ┆ f64     ┆   ┆ i32   ┆ i32        ┆ i64   ┆ list[i32]       │\n",
       "╞════════════╪═══════╪════════════════╪═════════╪═══╪═══════╪════════════╪═══════╪═════════════════╡\n",
       "│ 0          ┆ 60021 ┆ 1386547200     ┆ 4.0     ┆ … ┆ 217   ┆ 568        ┆ 1     ┆ [41675, 5429]   │\n",
       "│ 0          ┆ 61124 ┆ 1362268800     ┆ 5.0     ┆ … ┆ 1929  ┆ 459        ┆ 1     ┆ [59789]         │\n",
       "│ 0          ┆ 56065 ┆ 1355270400     ┆ 5.0     ┆ … ┆ 1223  ┆ 576        ┆ 1     ┆ [6154]          │\n",
       "│ 0          ┆ 59007 ┆ 1210204800     ┆ 5.0     ┆ … ┆ 1223  ┆ 607        ┆ 1     ┆ [4982, 61032, … │\n",
       "│            ┆       ┆                ┆         ┆   ┆       ┆            ┆       ┆ 15221]          │\n",
       "│ 0          ┆ 56272 ┆ 1387670400     ┆ 5.0     ┆ … ┆ 750   ┆ 425        ┆ 1     ┆ [43254]         │\n",
       "└────────────┴───────┴────────────────┴─────────┴───┴───────┴────────────┴───────┴─────────────────┘"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# train_dataset = Dataset.from_pandas(df_train).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)\n",
    "# test_dataset = Dataset.from_pandas(df_test).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)\n",
    "\n",
    "train_dataset = Dataset.from_polars(df_train).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)\n",
    "test_dataset = Dataset.from_polars(df_test).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchctr.functional import pad_sequences_to_maxlen\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # list of dict to dict of list\n",
    "    # print(batch)\n",
    "\n",
    "    batch_dense = []\n",
    "    batch_sparse = {}\n",
    "    for k in transformer.feat_configs:\n",
    "        if k['type'] == 'dense':\n",
    "            # print(k['name'])\n",
    "            batch_dense.append(\n",
    "                torch.tensor([sample[k['name']] for sample in batch], dtype=torch.float32)\n",
    "            )\n",
    "        elif k['type'] == 'sparse':\n",
    "            if k.get('islist'):\n",
    "                sparse_feat = [sample[k['name']] for sample in batch]\n",
    "                # pad sequences\n",
    "                sparse_feat = pad_sequences_to_maxlen(sparse_feat, batch_first=True, padding_value=-100, max_length=128)\n",
    "            else:\n",
    "                sparse_feat = torch.tensor([[sample[k['name']]] for sample in batch], dtype=torch.long)\n",
    "            batch_sparse[k['name']] = sparse_feat\n",
    "\n",
    "    batch_features = {\n",
    "        'dense_features': torch.stack(batch_dense, dim=1),\n",
    "        **batch_sparse\n",
    "    }\n",
    "    batch_labels = torch.tensor([[sample['label']] for sample in batch], dtype=torch.float32)\n",
    "    \n",
    "    return batch_features, batch_labels\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.1165],\n",
      "        [-0.4276]]), 'reviewerID': tensor([[148180],\n",
      "        [124848]]), 'asin': tensor([[31412],\n",
      "        [47720]]), 'brand': tensor([[1223],\n",
      "        [3027]]), 'categories': tensor([[302],\n",
      "        [557]]), 'his_asin_seq': tensor([[31468, 61039,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [43641, 29748,  1131,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])}\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(train_dataset, batch_size=2, num_workers=0, shuffle=True, collate_fn=collate_fn):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:24:05 torchctr INFO - Model Input: dense_size=1, sparse_size=67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(153923, 17)\n",
      "    (asin): Embedding(62384, 15)\n",
      "    (brand): Embedding(3504, 11)\n",
      "    (categories): Embedding(800, 9)\n",
      "    (his_asin_seq): Embedding(61926, 15)\n",
      "  )\n",
      "  (tower): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchctr.models import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(transformer.feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.002, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 15:24:45 torchctr INFO - [Validation] Epoch: 0/5, Validation Loss: {'loss': 0.6933948981001022}\n",
      "2024-11-11 15:24:45 torchctr INFO - Learning rate: 0.002\n",
      "2024-11-11 15:24:45 torchctr INFO - [Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.6966052651405334}\n",
      "2024-11-11 15:24:55 torchctr INFO - [Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.4531710368394852}\n",
      "2024-11-11 15:25:04 torchctr INFO - [Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.4170465312898159}\n",
      "2024-11-11 15:25:13 torchctr INFO - [Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.4033564534783363}\n",
      "2024-11-11 15:25:21 torchctr INFO - [Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.39582929089665414}\n",
      "2024-11-11 15:25:31 torchctr INFO - [Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.3896539005637169}\n",
      "2024-11-11 15:25:41 torchctr INFO - [Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.3855347283184528}\n",
      "2024-11-11 15:25:50 torchctr INFO - [Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.38281289211341313}\n",
      "2024-11-11 15:25:59 torchctr INFO - [Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.3800927373766899}\n",
      "2024-11-11 15:26:08 torchctr INFO - [Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.37793287853399915}\n",
      "2024-11-11 15:26:17 torchctr INFO - [Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.37563694816827775}\n",
      "2024-11-11 15:26:26 torchctr INFO - [Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.3737315033240752}\n",
      "2024-11-11 15:26:35 torchctr INFO - [Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.37215533514817556}\n",
      "2024-11-11 15:26:44 torchctr INFO - [Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.3709249773850808}\n",
      "2024-11-11 15:26:54 torchctr INFO - [Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.37034046081559996}\n",
      "2024-11-11 15:27:04 torchctr INFO - [Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.3691258177359899}\n",
      "2024-11-11 15:27:13 torchctr INFO - [Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.36837419748306277}\n",
      "2024-11-11 15:27:23 torchctr INFO - [Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.36765407516675835}\n",
      "2024-11-11 15:27:32 torchctr INFO - [Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.36690040118164485}\n",
      "2024-11-11 15:27:41 torchctr INFO - [Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.36614555710240415}\n",
      "2024-11-11 15:27:50 torchctr INFO - [Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.36547799709439277}\n",
      "2024-11-11 15:27:58 torchctr INFO - [Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.3648497992186319}\n",
      "2024-11-11 15:28:07 torchctr INFO - [Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.36414970060641116}\n",
      "2024-11-11 15:28:17 torchctr INFO - [Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.3635427642516468}\n",
      "2024-11-11 15:28:27 torchctr INFO - [Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.36292622634520133}\n",
      "2024-11-11 15:28:35 torchctr INFO - [Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.3623371989369392}\n",
      "2024-11-11 15:28:44 torchctr INFO - [Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.3617737864760252}\n",
      "2024-11-11 15:29:23 torchctr INFO - [Validation] Epoch: 1/5, Validation Loss: {'loss': 0.3448057140713405}\n",
      "2024-11-11 15:29:24 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.002642.ckpt\n",
      "2024-11-11 15:29:24 torchctr INFO - Learning rate: 0.0016\n",
      "2024-11-11 15:29:25 torchctr INFO - [Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.3735980987548828}\n",
      "2024-11-11 15:29:33 torchctr INFO - [Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.35225932151079176}\n",
      "2024-11-11 15:29:41 torchctr INFO - [Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.3481297770142555}\n",
      "2024-11-11 15:29:51 torchctr INFO - [Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.3466393961509069}\n",
      "2024-11-11 15:29:59 torchctr INFO - [Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.34691126115620136}\n",
      "2024-11-11 15:30:08 torchctr INFO - [Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.34546539551019667}\n",
      "2024-11-11 15:30:19 torchctr INFO - [Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.3453343938291073}\n",
      "2024-11-11 15:30:36 torchctr INFO - [Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.34488865341459}\n",
      "2024-11-11 15:30:44 torchctr INFO - [Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.34434142921119926}\n",
      "2024-11-11 15:30:50 torchctr INFO - [Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.34395535710785125}\n",
      "2024-11-11 15:30:57 torchctr INFO - [Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.34357317838072776}\n",
      "2024-11-11 15:31:04 torchctr INFO - [Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.3428599134900353}\n",
      "2024-11-11 15:31:13 torchctr INFO - [Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.34274411054948967}\n",
      "2024-11-11 15:31:19 torchctr INFO - [Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.3424395932830297}\n",
      "2024-11-11 15:31:27 torchctr INFO - [Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.34238144910761287}\n",
      "2024-11-11 15:31:34 torchctr INFO - [Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.3421128999590874}\n",
      "2024-11-11 15:31:42 torchctr INFO - [Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.3417447730153799}\n",
      "2024-11-11 15:31:49 torchctr INFO - [Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.34197329705252366}\n",
      "2024-11-11 15:31:55 torchctr INFO - [Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.34171707790758876}\n",
      "2024-11-11 15:32:01 torchctr INFO - [Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.3415756373813278}\n",
      "2024-11-11 15:32:07 torchctr INFO - [Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.34099204805493355}\n",
      "2024-11-11 15:32:12 torchctr INFO - [Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.34085513966424125}\n",
      "2024-11-11 15:32:18 torchctr INFO - [Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.3407405823333697}\n",
      "2024-11-11 15:32:23 torchctr INFO - [Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.3403505262084629}\n",
      "2024-11-11 15:32:28 torchctr INFO - [Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.34021317118157945}\n",
      "2024-11-11 15:32:34 torchctr INFO - [Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.34006169214248655}\n",
      "2024-11-11 15:32:39 torchctr INFO - [Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.33984745618242485}\n",
      "2024-11-11 15:33:02 torchctr INFO - [Validation] Epoch: 2/5, Validation Loss: {'loss': 0.3392762369660259}\n",
      "2024-11-11 15:33:03 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.005284.ckpt\n",
      "2024-11-11 15:33:03 torchctr INFO - Learning rate: 0.00128\n",
      "2024-11-11 15:33:04 torchctr INFO - [Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.2770061790943146}\n",
      "2024-11-11 15:33:09 torchctr INFO - [Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.3305350288748741}\n",
      "2024-11-11 15:33:15 torchctr INFO - [Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.325301136225462}\n",
      "2024-11-11 15:33:20 torchctr INFO - [Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.3244851813217004}\n",
      "2024-11-11 15:33:26 torchctr INFO - [Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.32435336634516715}\n",
      "2024-11-11 15:33:31 torchctr INFO - [Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.3234926281571388}\n",
      "2024-11-11 15:33:38 torchctr INFO - [Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.32311060617367426}\n",
      "2024-11-11 15:33:43 torchctr INFO - [Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.32208998418280055}\n",
      "2024-11-11 15:33:49 torchctr INFO - [Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.3215879460610449}\n",
      "2024-11-11 15:33:55 torchctr INFO - [Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.3214615815050072}\n",
      "2024-11-11 15:34:00 torchctr INFO - [Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.3213126933425665}\n",
      "2024-11-11 15:34:06 torchctr INFO - [Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.3207075764238834}\n",
      "2024-11-11 15:34:11 torchctr INFO - [Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.3205067536359032}\n",
      "2024-11-11 15:34:17 torchctr INFO - [Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.32019755855202675}\n",
      "2024-11-11 15:34:24 torchctr INFO - [Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32011226500783646}\n",
      "2024-11-11 15:34:31 torchctr INFO - [Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.31997080129384997}\n",
      "2024-11-11 15:34:37 torchctr INFO - [Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.3195756654627621}\n",
      "2024-11-11 15:34:44 torchctr INFO - [Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.3196018752806327}\n",
      "2024-11-11 15:34:51 torchctr INFO - [Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.31943029494749176}\n",
      "2024-11-11 15:34:58 torchctr INFO - [Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.31952676417011966}\n",
      "2024-11-11 15:35:03 torchctr INFO - [Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.3193639231994748}\n",
      "2024-11-11 15:35:12 torchctr INFO - [Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.3192434889645804}\n",
      "2024-11-11 15:35:19 torchctr INFO - [Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.3190268936753273}\n",
      "2024-11-11 15:35:26 torchctr INFO - [Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.3187557753531829}\n",
      "2024-11-11 15:35:32 torchctr INFO - [Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.31848927882189554}\n",
      "2024-11-11 15:35:40 torchctr INFO - [Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.3181656750202179}\n",
      "2024-11-11 15:35:47 torchctr INFO - [Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.3179744770377874}\n",
      "2024-11-11 15:36:14 torchctr INFO - [Validation] Epoch: 3/5, Validation Loss: {'loss': 0.3429561676315986}\n",
      "2024-11-11 15:36:15 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "2024-11-11 15:36:15 torchctr INFO - Learning rate: 0.0010240000000000002\n",
      "2024-11-11 15:36:16 torchctr INFO - [Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.3040509819984436}\n",
      "2024-11-11 15:36:23 torchctr INFO - [Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.29912167936563494}\n",
      "2024-11-11 15:36:31 torchctr INFO - [Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.29872785210609437}\n",
      "2024-11-11 15:36:38 torchctr INFO - [Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.29760709941387176}\n",
      "2024-11-11 15:36:44 torchctr INFO - [Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.2962228275835514}\n",
      "2024-11-11 15:36:51 torchctr INFO - [Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.29504649287462237}\n",
      "2024-11-11 15:36:59 torchctr INFO - [Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.2953087124725183}\n",
      "2024-11-11 15:37:06 torchctr INFO - [Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.2947505161379065}\n",
      "2024-11-11 15:37:13 torchctr INFO - [Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.294635417573154}\n",
      "2024-11-11 15:37:21 torchctr INFO - [Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.29459725244177715}\n",
      "2024-11-11 15:37:27 torchctr INFO - [Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.29435522578656675}\n",
      "2024-11-11 15:37:33 torchctr INFO - [Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.2944776459715583}\n",
      "2024-11-11 15:37:40 torchctr INFO - [Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.2945720679437121}\n",
      "2024-11-11 15:37:47 torchctr INFO - [Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.2943809830110807}\n",
      "2024-11-11 15:37:54 torchctr INFO - [Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.29439077545489584}\n",
      "2024-11-11 15:38:00 torchctr INFO - [Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.2943647092779477}\n",
      "2024-11-11 15:38:06 torchctr INFO - [Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.29411730393767355}\n",
      "2024-11-11 15:38:13 torchctr INFO - [Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.29424855879124473}\n",
      "2024-11-11 15:38:20 torchctr INFO - [Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.29402355693280696}\n",
      "2024-11-11 15:38:26 torchctr INFO - [Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.29403906901416027}\n",
      "2024-11-11 15:38:33 torchctr INFO - [Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.29392590972036126}\n",
      "2024-11-11 15:38:40 torchctr INFO - [Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.29384308962594896}\n",
      "2024-11-11 15:38:47 torchctr INFO - [Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.2938057764755054}\n",
      "2024-11-11 15:38:52 torchctr INFO - [Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.2937571068237657}\n",
      "2024-11-11 15:38:59 torchctr INFO - [Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.2938866158512731}\n",
      "2024-11-11 15:39:05 torchctr INFO - [Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.294073002499342}\n",
      "2024-11-11 15:39:11 torchctr INFO - [Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.2937255905339351}\n",
      "2024-11-11 15:39:38 torchctr INFO - [Validation] Epoch: 4/5, Validation Loss: {'loss': 0.354692216339807}\n",
      "2024-11-11 15:39:39 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "2024-11-11 15:39:39 torchctr INFO - Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from torchctr.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wsl/dev/torchctr/examples/../torchctr/trainer.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ckpt_file)\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded model state_dict from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded model.training from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded model.feat_configs from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x7f041abd1310> from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded logger = <Logger torchctr (INFO)> from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded ckpt_file_prefix = checkpoint from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded num_epoch = 4 from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded global_steps = 10568 from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded metadata_fn = ./ckpt//metadata.json from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded max_epochs = 5 from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "2024-11-11 15:39:40 torchctr INFO - Checkpoint loaded from ./ckpt/checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('./ckpt')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6742899835345096\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
