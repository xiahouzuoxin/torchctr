{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchctr_root = '../'\n",
    "\n",
    "import sys\n",
    "sys.path.append(torchctr_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "df_samples = joblib.load(f'{torchctr_root}/data/amazon_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192403"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['reviewerID'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63001"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['asin'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3526"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['brand'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( df_samples['categories'].unique() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence features from latest to oldest\n",
    "df_samples['his_asin_seq'] = df_samples['his_asin_seq'].map(lambda x: x[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'reviewerID', 'dtype': 'category', 'emb_dim': 17, 'min_freq': 3}, {'name': 'asin', 'dtype': 'category', 'emb_dim': 15, 'min_freq': 3}, {'name': 'price', 'dtype': 'numerical', 'norm': 'std', 'mean': np.float64(74.40153304932919), 'std': np.float64(123.75264929565961)}, {'name': 'brand', 'dtype': 'category', 'emb_dim': 11, 'min_freq': 3}, {'name': 'categories', 'dtype': 'category', 'emb_dim': 9, 'min_freq': 3}, {'name': 'his_asin_seq', 'dtype': 'category', 'islist': True, 'emb_dim': 15, 'min_freq': 3, 'max_len': 256}]\n"
     ]
    }
   ],
   "source": [
    "## Hash buckets\n",
    "# feat_configs = [\n",
    "#     {\"name\": \"reviewerID\", \"dtype\": \"category\", \"emb_dim\": 17,  \"hash_buckets\": 'auto'},\n",
    "#     {\"name\": \"asin\", \"dtype\": \"category\", \"emb_dim\": 15, 'min_freq': 3},\n",
    "    \n",
    "#     {\"name\": \"price\", \"dtype\": \"numerical\", \"norm\": 'std'},\n",
    "#     {\"name\": \"brand\", \"dtype\": \"category\",  \"emb_dim\": 12, 'min_freq': 3},\n",
    "#     {\"name\": \"categories\", \"dtype\": \"category\",  \"emb_dim\": 12, 'min_freq': 3},\n",
    "\n",
    "#     {\"name\": \"his_asin_seq\", \"dtype\": \"category\", \"emb_dim\": 15, 'min_freq': 3, \"islist\": True, \"maxlen\": 256},\n",
    "# ]\n",
    "\n",
    "## Auto generate feat_configs\n",
    "from torchctr.utils import auto_generate_feature_configs\n",
    "feat_configs = auto_generate_feature_configs(\n",
    "    df_samples[['reviewerID', 'asin', 'price', 'brand', 'categories', 'his_asin_seq']]\n",
    ")\n",
    "\n",
    "print(feat_configs)\n",
    "\n",
    "target_cols = ['label', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "df_samples = pl.from_pandas(df_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:37:17 torchctr WARNING - Feature configurations only contain column names, auto-generating feature configurations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1351158, 10) (338030, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 18:37:18 torchctr INFO - Auto-generated feature configurations: [{'name': 'reviewerID', 'dtype': 'category', 'emb_dim': 17, 'min_freq': 3}, {'name': 'asin', 'dtype': 'category', 'emb_dim': 15, 'min_freq': 3}, {'name': 'price', 'dtype': 'numerical', 'norm': 'std', 'mean': 74.4471722884851, 'std': 123.8221824638805}, {'name': 'brand', 'dtype': 'category', 'emb_dim': 11, 'min_freq': 3}, {'name': 'categories', 'dtype': 'category', 'emb_dim': 9, 'min_freq': 3}, {'name': 'his_asin_seq', 'dtype': 'category', 'islist': True, 'max_len': 256, 'emb_dim': 15, 'min_freq': 3}]\n",
      "2024-11-15 18:37:18 torchctr INFO - Processing feature reviewerID...\n",
      "2024-11-15 18:37:18 torchctr INFO - Feature reviewerID vocab size: None -> 153923\n",
      "2024-11-15 18:37:18 torchctr INFO - Processing feature asin...\n",
      "2024-11-15 18:37:18 torchctr INFO - Feature asin vocab size: None -> 62333\n",
      "2024-11-15 18:37:18 torchctr INFO - Processing feature price...\n",
      "2024-11-15 18:37:18 torchctr INFO - Feature price updated: mean=74.4471722884851, std=123.8221824638805, min=0.01, max=999.99\n",
      "2024-11-15 18:37:18 torchctr INFO - Processing feature brand...\n",
      "2024-11-15 18:37:19 torchctr INFO - Feature brand vocab size: None -> 3509\n",
      "2024-11-15 18:37:19 torchctr INFO - Processing feature categories...\n",
      "2024-11-15 18:37:19 torchctr INFO - Feature categories vocab size: None -> 801\n",
      "2024-11-15 18:37:19 torchctr INFO - Processing feature his_asin_seq...\n",
      "2024-11-15 18:37:20 torchctr INFO - Feature his_asin_seq vocab size: None -> 61934\n",
      "2024-11-15 18:37:21 torchctr INFO - Processing feature reviewerID...\n",
      "2024-11-15 18:37:21 torchctr INFO - Processing feature asin...\n",
      "2024-11-15 18:37:21 torchctr INFO - Processing feature price...\n",
      "2024-11-15 18:37:21 torchctr INFO - Processing feature brand...\n",
      "2024-11-15 18:37:21 torchctr INFO - Processing feature categories...\n",
      "2024-11-15 18:37:21 torchctr INFO - Processing feature his_asin_seq...\n"
     ]
    }
   ],
   "source": [
    "from torchctr.transformer import FeatureTransformer\n",
    "\n",
    "transformer = FeatureTransformer(feat_configs=['reviewerID', 'asin', 'price', 'brand', 'categories', 'his_asin_seq'], category_min_freq=3, verbose=True) # will auto generate other feat_configs \n",
    "df_train, df_test = transformer.split(df_samples, test_size=0.2, shuffle=True, group_col='reviewerID')\n",
    "print(df_train.shape, df_test.shape)\n",
    "\n",
    "df_train = transformer.fit_transform(df_train)\n",
    "df_test = transformer.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>reviewerID</th><th>asin</th><th>unixReviewTime</th><th>overall</th><th>title</th><th>price</th><th>brand</th><th>categories</th><th>label</th><th>his_asin_seq</th><th>_index</th></tr><tr><td>u32</td><td>u32</td><td>i64</td><td>f64</td><td>str</td><td>f64</td><td>u32</td><td>u32</td><td>i64</td><td>list[u32]</td><td>i64</td></tr></thead><tbody><tr><td>0</td><td>44949</td><td>1359331200</td><td>5.0</td><td>&quot;AMD FD8350FRHKBOX FX-8350  FX-…</td><td>0.922561</td><td>2553</td><td>108</td><td>1</td><td>[]</td><td>0</td></tr><tr><td>0</td><td>45173</td><td>1361145600</td><td>4.0</td><td>&quot;TRENDnet 1-Port Print Server T…</td><td>-0.261893</td><td>719</td><td>27</td><td>1</td><td>[39436]</td><td>1</td></tr><tr><td>0</td><td>30796</td><td>1361145600</td><td>4.0</td><td>null</td><td>0.206122</td><td>719</td><td>700</td><td>1</td><td>[39436, 61681]</td><td>2</td></tr><tr><td>0</td><td>19417</td><td>1361145600</td><td>5.0</td><td>&quot;Thermaltake Water 2.0 PRO/All …</td><td>-0.537453</td><td>719</td><td>80</td><td>1</td><td>[39436, 61681, 45308]</td><td>3</td></tr><tr><td>0</td><td>36971</td><td>1389744000</td><td>5.0</td><td>&quot;Replacement Battery for PLANTR…</td><td>-0.521381</td><td>3065</td><td>363</td><td>1</td><td>[39436, 61681, … 42890]</td><td>4</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 11)\n",
       "┌────────────┬───────┬────────────────┬─────────┬───┬────────────┬───────┬────────────────┬────────┐\n",
       "│ reviewerID ┆ asin  ┆ unixReviewTime ┆ overall ┆ … ┆ categories ┆ label ┆ his_asin_seq   ┆ _index │\n",
       "│ ---        ┆ ---   ┆ ---            ┆ ---     ┆   ┆ ---        ┆ ---   ┆ ---            ┆ ---    │\n",
       "│ u32        ┆ u32   ┆ i64            ┆ f64     ┆   ┆ u32        ┆ i64   ┆ list[u32]      ┆ i64    │\n",
       "╞════════════╪═══════╪════════════════╪═════════╪═══╪════════════╪═══════╪════════════════╪════════╡\n",
       "│ 0          ┆ 44949 ┆ 1359331200     ┆ 5.0     ┆ … ┆ 108        ┆ 1     ┆ []             ┆ 0      │\n",
       "│ 0          ┆ 45173 ┆ 1361145600     ┆ 4.0     ┆ … ┆ 27         ┆ 1     ┆ [39436]        ┆ 1      │\n",
       "│ 0          ┆ 30796 ┆ 1361145600     ┆ 4.0     ┆ … ┆ 700        ┆ 1     ┆ [39436, 61681] ┆ 2      │\n",
       "│ 0          ┆ 19417 ┆ 1361145600     ┆ 5.0     ┆ … ┆ 80         ┆ 1     ┆ [39436, 61681, ┆ 3      │\n",
       "│            ┆       ┆                ┆         ┆   ┆            ┆       ┆ 45308]         ┆        │\n",
       "│ 0          ┆ 36971 ┆ 1389744000     ┆ 5.0     ┆ … ┆ 363        ┆ 1     ┆ [39436, 61681, ┆ 4      │\n",
       "│            ┆       ┆                ┆         ┆   ┆            ┆       ┆ … 42890]       ┆        │\n",
       "└────────────┴───────┴────────────────┴─────────┴───┴────────────┴───────┴────────────────┴────────┘"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# train_dataset = Dataset.from_pandas(df_train).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)\n",
    "# test_dataset = Dataset.from_pandas(df_test).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)\n",
    "\n",
    "train_dataset = Dataset.from_polars(df_train).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)\n",
    "test_dataset = Dataset.from_polars(df_test).with_format('torch', columns=[x['name'] for x in feat_configs] + target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchctr.nn.functional import pad_sequences_to_maxlen\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # list of dict to dict of list\n",
    "    # print(batch)\n",
    "\n",
    "    batch_dense = []\n",
    "    batch_sparse = {}\n",
    "    for k in transformer.feat_configs:\n",
    "        if k['type'] == 'dense':\n",
    "            # print(k['name'])\n",
    "            batch_dense.append(\n",
    "                torch.tensor([sample[k['name']] for sample in batch], dtype=torch.float32)\n",
    "            )\n",
    "        elif k['type'] == 'sparse':\n",
    "            if k.get('islist'):\n",
    "                sparse_feat = [sample[k['name']] for sample in batch]\n",
    "                # pad sequences\n",
    "                sparse_feat = pad_sequences_to_maxlen(sparse_feat, batch_first=True, padding_value=-100, max_length=128)\n",
    "            else:\n",
    "                sparse_feat = torch.tensor([[sample[k['name']]] for sample in batch], dtype=torch.long)\n",
    "            batch_sparse[k['name']] = sparse_feat\n",
    "\n",
    "    batch_features = {\n",
    "        'dense_features': torch.stack(batch_dense, dim=1),\n",
    "        **batch_sparse\n",
    "    }\n",
    "    batch_labels = torch.tensor([[sample['label']] for sample in batch], dtype=torch.float32)\n",
    "    \n",
    "    return batch_features, batch_labels\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=512, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2642\n",
      "{'dense_features': tensor([[-0.4717],\n",
      "        [ 3.0279]]), 'reviewerID': tensor([[0],\n",
      "        [0]]), 'asin': tensor([[15335],\n",
      "        [28189]]), 'brand': tensor([[2673],\n",
      "        [ 982]]), 'categories': tensor([[260],\n",
      "        [222]]), 'his_asin_seq': tensor([[22703,  8495, 28753,  8802, 36551, 47189,  8762, 31913, 53720,  8971,\n",
      "         22570, 48729, 34835, 46590, 33885,  4333, 53178,  5877,  9980, 33010,\n",
      "         56951, 49439, 59185, 35703, 20967, 59088, 56510, 49491, 15631, 58362,\n",
      "         12246,  1024, 17604, 30655, 33966, 56054,  8261, 60505, 57148, 35790,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100],\n",
      "        [ 2396,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])}\n",
      "tensor([[1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print( len(train_dataloader) )\n",
    "for features, labels in DataLoader(test_dataset, batch_size=2, num_workers=0, shuffle=True, collate_fn=collate_fn):\n",
    "    print(features)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 16:31:57 torchctr INFO - Model Input: dense_size=1, sparse_size=67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN(\n",
      "  (embeddings): ModuleDict(\n",
      "    (reviewerID): Embedding(153923, 17)\n",
      "    (asin): Embedding(62384, 15)\n",
      "    (brand): Embedding(3504, 11)\n",
      "    (categories): Embedding(800, 9)\n",
      "    (his_asin_seq): Embedding(61926, 15)\n",
      "  )\n",
      "  (tower): Sequential(\n",
      "    (0): Linear(in_features=68, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torchctr.models import DNN\n",
    "\n",
    "dnn_hidden_units = [128,64,32]\n",
    "model = DNN(transformer.feat_configs, hidden_units=dnn_hidden_units)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),  lr = 0.002, weight_decay = 1e-9)\n",
    "lr_scd = lr_scheduler.StepLR(optimizer, step_size=len(train_dataloader), gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 16:32:29 torchctr INFO - [Validation] Epoch: 0/5, Validation Loss: {'loss': 0.7817239257702349}\n",
      "2024-11-15 16:32:29 torchctr INFO - Learning rate: 0.002\n",
      "2024-11-15 16:32:29 torchctr INFO - [Training] Epoch: 1/5 iter 0/2642, Training Loss: {'loss': 0.8893228769302368}\n",
      "2024-11-15 16:32:36 torchctr INFO - [Training] Epoch: 1/5 iter 100/2642, Training Loss: {'loss': 0.5205774176120758}\n",
      "2024-11-15 16:32:43 torchctr INFO - [Training] Epoch: 1/5 iter 200/2642, Training Loss: {'loss': 0.45305554926395414}\n",
      "2024-11-15 16:32:51 torchctr INFO - [Training] Epoch: 1/5 iter 300/2642, Training Loss: {'loss': 0.42856981287399926}\n",
      "2024-11-15 16:32:58 torchctr INFO - [Training] Epoch: 1/5 iter 400/2642, Training Loss: {'loss': 0.4170663586258888}\n",
      "2024-11-15 16:33:05 torchctr INFO - [Training] Epoch: 1/5 iter 500/2642, Training Loss: {'loss': 0.4071468803882599}\n",
      "2024-11-15 16:33:12 torchctr INFO - [Training] Epoch: 1/5 iter 600/2642, Training Loss: {'loss': 0.40109682058294616}\n",
      "2024-11-15 16:33:19 torchctr INFO - [Training] Epoch: 1/5 iter 700/2642, Training Loss: {'loss': 0.3960344099146979}\n",
      "2024-11-15 16:33:27 torchctr INFO - [Training] Epoch: 1/5 iter 800/2642, Training Loss: {'loss': 0.39207770325243474}\n",
      "2024-11-15 16:33:34 torchctr INFO - [Training] Epoch: 1/5 iter 900/2642, Training Loss: {'loss': 0.3891692132751147}\n",
      "2024-11-15 16:33:41 torchctr INFO - [Training] Epoch: 1/5 iter 1000/2642, Training Loss: {'loss': 0.3867536550164223}\n",
      "2024-11-15 16:33:48 torchctr INFO - [Training] Epoch: 1/5 iter 1100/2642, Training Loss: {'loss': 0.38486771821975707}\n",
      "2024-11-15 16:33:55 torchctr INFO - [Training] Epoch: 1/5 iter 1200/2642, Training Loss: {'loss': 0.38260950523118176}\n",
      "2024-11-15 16:34:02 torchctr INFO - [Training] Epoch: 1/5 iter 1300/2642, Training Loss: {'loss': 0.38077876366101776}\n",
      "2024-11-15 16:34:09 torchctr INFO - [Training] Epoch: 1/5 iter 1400/2642, Training Loss: {'loss': 0.37926029524632865}\n",
      "2024-11-15 16:34:15 torchctr INFO - [Training] Epoch: 1/5 iter 1500/2642, Training Loss: {'loss': 0.37769322272141775}\n",
      "2024-11-15 16:34:21 torchctr INFO - [Training] Epoch: 1/5 iter 1600/2642, Training Loss: {'loss': 0.3763052249886096}\n",
      "2024-11-15 16:34:28 torchctr INFO - [Training] Epoch: 1/5 iter 1700/2642, Training Loss: {'loss': 0.3749857761754709}\n",
      "2024-11-15 16:34:35 torchctr INFO - [Training] Epoch: 1/5 iter 1800/2642, Training Loss: {'loss': 0.37387855225139194}\n",
      "2024-11-15 16:34:42 torchctr INFO - [Training] Epoch: 1/5 iter 1900/2642, Training Loss: {'loss': 0.37264407656694715}\n",
      "2024-11-15 16:34:48 torchctr INFO - [Training] Epoch: 1/5 iter 2000/2642, Training Loss: {'loss': 0.37180340172350407}\n",
      "2024-11-15 16:34:54 torchctr INFO - [Training] Epoch: 1/5 iter 2100/2642, Training Loss: {'loss': 0.37089182667789006}\n",
      "2024-11-15 16:35:01 torchctr INFO - [Training] Epoch: 1/5 iter 2200/2642, Training Loss: {'loss': 0.36989902275529774}\n",
      "2024-11-15 16:35:09 torchctr INFO - [Training] Epoch: 1/5 iter 2300/2642, Training Loss: {'loss': 0.36888991762762485}\n",
      "2024-11-15 16:35:14 torchctr INFO - [Training] Epoch: 1/5 iter 2400/2642, Training Loss: {'loss': 0.3679844461381435}\n",
      "2024-11-15 16:35:21 torchctr INFO - [Training] Epoch: 1/5 iter 2500/2642, Training Loss: {'loss': 0.3671230372548103}\n",
      "2024-11-15 16:35:27 torchctr INFO - [Training] Epoch: 1/5 iter 2600/2642, Training Loss: {'loss': 0.3663897030284772}\n",
      "2024-11-15 16:35:55 torchctr INFO - [Validation] Epoch: 1/5, Validation Loss: {'loss': 0.34304737786572753}\n",
      "2024-11-15 16:35:55 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.002642.ckpt\n",
      "2024-11-15 16:35:55 torchctr INFO - Learning rate: 0.0016\n",
      "2024-11-15 16:35:56 torchctr INFO - [Training] Epoch: 2/5 iter 0/2642, Training Loss: {'loss': 0.3887444734573364}\n",
      "2024-11-15 16:36:05 torchctr INFO - [Training] Epoch: 2/5 iter 100/2642, Training Loss: {'loss': 0.3497086584568024}\n",
      "2024-11-15 16:36:13 torchctr INFO - [Training] Epoch: 2/5 iter 200/2642, Training Loss: {'loss': 0.3442747376859188}\n",
      "2024-11-15 16:36:20 torchctr INFO - [Training] Epoch: 2/5 iter 300/2642, Training Loss: {'loss': 0.3460887424151103}\n",
      "2024-11-15 16:36:28 torchctr INFO - [Training] Epoch: 2/5 iter 400/2642, Training Loss: {'loss': 0.34419459745287895}\n",
      "2024-11-15 16:36:35 torchctr INFO - [Training] Epoch: 2/5 iter 500/2642, Training Loss: {'loss': 0.3443865576386452}\n",
      "2024-11-15 16:36:42 torchctr INFO - [Training] Epoch: 2/5 iter 600/2642, Training Loss: {'loss': 0.34498458554347355}\n",
      "2024-11-15 16:36:48 torchctr INFO - [Training] Epoch: 2/5 iter 700/2642, Training Loss: {'loss': 0.3440968235901424}\n",
      "2024-11-15 16:36:54 torchctr INFO - [Training] Epoch: 2/5 iter 800/2642, Training Loss: {'loss': 0.3438578801229596}\n",
      "2024-11-15 16:37:00 torchctr INFO - [Training] Epoch: 2/5 iter 900/2642, Training Loss: {'loss': 0.3434242623713281}\n",
      "2024-11-15 16:37:08 torchctr INFO - [Training] Epoch: 2/5 iter 1000/2642, Training Loss: {'loss': 0.3426625472903252}\n",
      "2024-11-15 16:37:14 torchctr INFO - [Training] Epoch: 2/5 iter 1100/2642, Training Loss: {'loss': 0.34264527001164174}\n",
      "2024-11-15 16:37:21 torchctr INFO - [Training] Epoch: 2/5 iter 1200/2642, Training Loss: {'loss': 0.3421313492332896}\n",
      "2024-11-15 16:37:28 torchctr INFO - [Training] Epoch: 2/5 iter 1300/2642, Training Loss: {'loss': 0.3422833539087039}\n",
      "2024-11-15 16:37:34 torchctr INFO - [Training] Epoch: 2/5 iter 1400/2642, Training Loss: {'loss': 0.3419240009891135}\n",
      "2024-11-15 16:37:41 torchctr INFO - [Training] Epoch: 2/5 iter 1500/2642, Training Loss: {'loss': 0.3418769828577836}\n",
      "2024-11-15 16:37:47 torchctr INFO - [Training] Epoch: 2/5 iter 1600/2642, Training Loss: {'loss': 0.34168011472560467}\n",
      "2024-11-15 16:37:53 torchctr INFO - [Training] Epoch: 2/5 iter 1700/2642, Training Loss: {'loss': 0.3415400071617435}\n",
      "2024-11-15 16:38:00 torchctr INFO - [Training] Epoch: 2/5 iter 1800/2642, Training Loss: {'loss': 0.3413282625956668}\n",
      "2024-11-15 16:38:06 torchctr INFO - [Training] Epoch: 2/5 iter 1900/2642, Training Loss: {'loss': 0.3410783190240985}\n",
      "2024-11-15 16:38:13 torchctr INFO - [Training] Epoch: 2/5 iter 2000/2642, Training Loss: {'loss': 0.3407388147637248}\n",
      "2024-11-15 16:38:20 torchctr INFO - [Training] Epoch: 2/5 iter 2100/2642, Training Loss: {'loss': 0.34045387236135344}\n",
      "2024-11-15 16:38:27 torchctr INFO - [Training] Epoch: 2/5 iter 2200/2642, Training Loss: {'loss': 0.3402130475166169}\n",
      "2024-11-15 16:38:33 torchctr INFO - [Training] Epoch: 2/5 iter 2300/2642, Training Loss: {'loss': 0.34014871702893923}\n",
      "2024-11-15 16:38:39 torchctr INFO - [Training] Epoch: 2/5 iter 2400/2642, Training Loss: {'loss': 0.3399231552022199}\n",
      "2024-11-15 16:38:45 torchctr INFO - [Training] Epoch: 2/5 iter 2500/2642, Training Loss: {'loss': 0.3396895404160023}\n",
      "2024-11-15 16:38:51 torchctr INFO - [Training] Epoch: 2/5 iter 2600/2642, Training Loss: {'loss': 0.3395802452873725}\n",
      "2024-11-15 16:39:19 torchctr INFO - [Validation] Epoch: 2/5, Validation Loss: {'loss': 0.33641604576429696}\n",
      "2024-11-15 16:39:20 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.005284.ckpt\n",
      "2024-11-15 16:39:20 torchctr INFO - Learning rate: 0.00128\n",
      "2024-11-15 16:39:21 torchctr INFO - [Training] Epoch: 3/5 iter 0/2642, Training Loss: {'loss': 0.3294488787651062}\n",
      "2024-11-15 16:39:27 torchctr INFO - [Training] Epoch: 3/5 iter 100/2642, Training Loss: {'loss': 0.33164445132017134}\n",
      "2024-11-15 16:39:33 torchctr INFO - [Training] Epoch: 3/5 iter 200/2642, Training Loss: {'loss': 0.3281219157576561}\n",
      "2024-11-15 16:39:39 torchctr INFO - [Training] Epoch: 3/5 iter 300/2642, Training Loss: {'loss': 0.32567978769540784}\n",
      "2024-11-15 16:39:45 torchctr INFO - [Training] Epoch: 3/5 iter 400/2642, Training Loss: {'loss': 0.325767232850194}\n",
      "2024-11-15 16:39:52 torchctr INFO - [Training] Epoch: 3/5 iter 500/2642, Training Loss: {'loss': 0.3246681326627731}\n",
      "2024-11-15 16:40:01 torchctr INFO - [Training] Epoch: 3/5 iter 600/2642, Training Loss: {'loss': 0.324194725950559}\n",
      "2024-11-15 16:40:08 torchctr INFO - [Training] Epoch: 3/5 iter 700/2642, Training Loss: {'loss': 0.3229489170227732}\n",
      "2024-11-15 16:40:16 torchctr INFO - [Training] Epoch: 3/5 iter 800/2642, Training Loss: {'loss': 0.32282925840467214}\n",
      "2024-11-15 16:40:22 torchctr INFO - [Training] Epoch: 3/5 iter 900/2642, Training Loss: {'loss': 0.3226784069008297}\n",
      "2024-11-15 16:40:29 torchctr INFO - [Training] Epoch: 3/5 iter 1000/2642, Training Loss: {'loss': 0.32245811985433104}\n",
      "2024-11-15 16:40:35 torchctr INFO - [Training] Epoch: 3/5 iter 1100/2642, Training Loss: {'loss': 0.3224592931839553}\n",
      "2024-11-15 16:40:44 torchctr INFO - [Training] Epoch: 3/5 iter 1200/2642, Training Loss: {'loss': 0.3221473453814785}\n",
      "2024-11-15 16:40:49 torchctr INFO - [Training] Epoch: 3/5 iter 1300/2642, Training Loss: {'loss': 0.3217589548115547}\n",
      "2024-11-15 16:40:56 torchctr INFO - [Training] Epoch: 3/5 iter 1400/2642, Training Loss: {'loss': 0.32171822227537633}\n",
      "2024-11-15 16:41:03 torchctr INFO - [Training] Epoch: 3/5 iter 1500/2642, Training Loss: {'loss': 0.3216266629397869}\n",
      "2024-11-15 16:41:10 torchctr INFO - [Training] Epoch: 3/5 iter 1600/2642, Training Loss: {'loss': 0.3214370645955205}\n",
      "2024-11-15 16:41:18 torchctr INFO - [Training] Epoch: 3/5 iter 1700/2642, Training Loss: {'loss': 0.32111535592114226}\n",
      "2024-11-15 16:41:25 torchctr INFO - [Training] Epoch: 3/5 iter 1800/2642, Training Loss: {'loss': 0.32081403558452926}\n",
      "2024-11-15 16:41:33 torchctr INFO - [Training] Epoch: 3/5 iter 1900/2642, Training Loss: {'loss': 0.3205288876985249}\n",
      "2024-11-15 16:41:39 torchctr INFO - [Training] Epoch: 3/5 iter 2000/2642, Training Loss: {'loss': 0.32011740881949663}\n",
      "2024-11-15 16:41:47 torchctr INFO - [Training] Epoch: 3/5 iter 2100/2642, Training Loss: {'loss': 0.3201071636094934}\n",
      "2024-11-15 16:41:54 torchctr INFO - [Training] Epoch: 3/5 iter 2200/2642, Training Loss: {'loss': 0.3199480037662116}\n",
      "2024-11-15 16:42:00 torchctr INFO - [Training] Epoch: 3/5 iter 2300/2642, Training Loss: {'loss': 0.3196646811068058}\n",
      "2024-11-15 16:42:08 torchctr INFO - [Training] Epoch: 3/5 iter 2400/2642, Training Loss: {'loss': 0.3196922523714602}\n",
      "2024-11-15 16:42:14 torchctr INFO - [Training] Epoch: 3/5 iter 2500/2642, Training Loss: {'loss': 0.31948452664613725}\n",
      "2024-11-15 16:42:23 torchctr INFO - [Training] Epoch: 3/5 iter 2600/2642, Training Loss: {'loss': 0.3193450739521247}\n",
      "2024-11-15 16:42:54 torchctr INFO - [Validation] Epoch: 3/5, Validation Loss: {'loss': 0.3364343441184893}\n",
      "2024-11-15 16:42:55 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.007926.ckpt\n",
      "2024-11-15 16:42:55 torchctr INFO - Learning rate: 0.0010240000000000002\n",
      "2024-11-15 16:42:56 torchctr INFO - [Training] Epoch: 4/5 iter 0/2642, Training Loss: {'loss': 0.2908385694026947}\n",
      "2024-11-15 16:43:03 torchctr INFO - [Training] Epoch: 4/5 iter 100/2642, Training Loss: {'loss': 0.3057593494653702}\n",
      "2024-11-15 16:43:11 torchctr INFO - [Training] Epoch: 4/5 iter 200/2642, Training Loss: {'loss': 0.30128641679883006}\n",
      "2024-11-15 16:43:17 torchctr INFO - [Training] Epoch: 4/5 iter 300/2642, Training Loss: {'loss': 0.299574855218331}\n",
      "2024-11-15 16:43:23 torchctr INFO - [Training] Epoch: 4/5 iter 400/2642, Training Loss: {'loss': 0.2988848667219281}\n",
      "2024-11-15 16:43:30 torchctr INFO - [Training] Epoch: 4/5 iter 500/2642, Training Loss: {'loss': 0.2987254965305328}\n",
      "2024-11-15 16:43:37 torchctr INFO - [Training] Epoch: 4/5 iter 600/2642, Training Loss: {'loss': 0.29823202691972256}\n",
      "2024-11-15 16:43:43 torchctr INFO - [Training] Epoch: 4/5 iter 700/2642, Training Loss: {'loss': 0.2980219880597932}\n",
      "2024-11-15 16:43:50 torchctr INFO - [Training] Epoch: 4/5 iter 800/2642, Training Loss: {'loss': 0.29818308994174003}\n",
      "2024-11-15 16:43:58 torchctr INFO - [Training] Epoch: 4/5 iter 900/2642, Training Loss: {'loss': 0.29829558660586675}\n",
      "2024-11-15 16:44:05 torchctr INFO - [Training] Epoch: 4/5 iter 1000/2642, Training Loss: {'loss': 0.29806835335493087}\n",
      "2024-11-15 16:44:11 torchctr INFO - [Training] Epoch: 4/5 iter 1100/2642, Training Loss: {'loss': 0.2982238356091759}\n",
      "2024-11-15 16:44:19 torchctr INFO - [Training] Epoch: 4/5 iter 1200/2642, Training Loss: {'loss': 0.2981954079742233}\n",
      "2024-11-15 16:44:25 torchctr INFO - [Training] Epoch: 4/5 iter 1300/2642, Training Loss: {'loss': 0.29792383323495203}\n",
      "2024-11-15 16:44:32 torchctr INFO - [Training] Epoch: 4/5 iter 1400/2642, Training Loss: {'loss': 0.2974564109849078}\n",
      "2024-11-15 16:44:38 torchctr INFO - [Training] Epoch: 4/5 iter 1500/2642, Training Loss: {'loss': 0.29703312571843465}\n",
      "2024-11-15 16:44:46 torchctr INFO - [Training] Epoch: 4/5 iter 1600/2642, Training Loss: {'loss': 0.2966615021042526}\n",
      "2024-11-15 16:44:52 torchctr INFO - [Training] Epoch: 4/5 iter 1700/2642, Training Loss: {'loss': 0.29665569259839897}\n",
      "2024-11-15 16:44:59 torchctr INFO - [Training] Epoch: 4/5 iter 1800/2642, Training Loss: {'loss': 0.2967398322373629}\n",
      "2024-11-15 16:45:06 torchctr INFO - [Training] Epoch: 4/5 iter 1900/2642, Training Loss: {'loss': 0.29678182870933884}\n",
      "2024-11-15 16:45:12 torchctr INFO - [Training] Epoch: 4/5 iter 2000/2642, Training Loss: {'loss': 0.2967124779224396}\n",
      "2024-11-15 16:45:19 torchctr INFO - [Training] Epoch: 4/5 iter 2100/2642, Training Loss: {'loss': 0.29653358208281655}\n",
      "2024-11-15 16:45:25 torchctr INFO - [Training] Epoch: 4/5 iter 2200/2642, Training Loss: {'loss': 0.29615661672570487}\n",
      "2024-11-15 16:45:33 torchctr INFO - [Training] Epoch: 4/5 iter 2300/2642, Training Loss: {'loss': 0.29609846673581913}\n",
      "2024-11-15 16:45:40 torchctr INFO - [Training] Epoch: 4/5 iter 2400/2642, Training Loss: {'loss': 0.29604444921637574}\n",
      "2024-11-15 16:45:47 torchctr INFO - [Training] Epoch: 4/5 iter 2500/2642, Training Loss: {'loss': 0.2958414246618748}\n",
      "2024-11-15 16:45:54 torchctr INFO - [Training] Epoch: 4/5 iter 2600/2642, Training Loss: {'loss': 0.2957415090443996}\n",
      "2024-11-15 16:46:28 torchctr INFO - [Validation] Epoch: 4/5, Validation Loss: {'loss': 0.3460519670715448}\n",
      "2024-11-15 16:46:29 torchctr INFO - Checkpoint saved at ./ckpt//checkpoint.010568.ckpt\n",
      "2024-11-15 16:46:29 torchctr INFO - Early stopping at epoch 4...\n"
     ]
    }
   ],
   "source": [
    "from torchctr.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scd,\n",
    "    max_epochs=5,\n",
    "    early_stopping_rounds=3,\n",
    "    save_ckpt_path='./ckpt/'\n",
    ")\n",
    "\n",
    "model = trainer.fit(train_dataloader, eval_dataloader = test_dataloader, ret_model = 'final') #, init_ckpt_path='./ckpt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wsl/dev/torchctr/examples/../torchctr/trainer.py:295: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  \n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded model state_dict from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded model.training from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded model.feat_configs from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded optimizer = Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differ... from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded lr_scheduler = <torch.optim.lr_scheduler.StepLR object at 0x7f1ebca7bef0> from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded logger = <Logger torchctr (INFO)> from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded ckpt_file_prefix = checkpoint from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded num_epoch = 4 from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded global_steps = 10568 from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded save_ckpt_path = ./ckpt/ from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded metadata_fn = ./ckpt//metadata.json from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded save_ckpt_steps = epoch from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded max_epochs = 5 from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Loaded early_stopping_rounds = 3 from checkpoint.\n",
      "2024-11-15 16:46:29 torchctr INFO - Checkpoint loaded from ./ckpt/checkpoint.010568.ckpt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = trainer.load_ckpt('./ckpt')\n",
    "model.load_state_dict(ckpt['model'].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "test_labels = []\n",
    "model.eval()\n",
    "\n",
    "for features, labels in test_dataloader:\n",
    "    outputs = model(features)\n",
    "    test_preds.append(outputs[:,0])\n",
    "    test_labels.append(labels[:,0])\n",
    "test_preds = torch.concat(test_preds, dim=0).detach().cpu().numpy()\n",
    "test_labels = torch.concat(test_labels, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336650,) (336650,)\n"
     ]
    }
   ],
   "source": [
    "print(test_preds.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.6764908327220651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(test_labels, test_preds)\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Test service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status\":\"ok\"}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0   2461      0 --:--:-- --:--:-- --:--:--  3000\n"
     ]
    }
   ],
   "source": [
    "# Run this on terminal under root of project\n",
    "# !python -m torchctr.serving.serve --name dnn --path examples/ckpt/checkpoint.010568.ckpt --dep_paths examples\n",
    "\n",
    "import os\n",
    "ret = os.system('curl http://localhost:8000/dnn/health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predict(df, name):\n",
    "    ''' \n",
    "    Test the prediction of the model.\n",
    "    First launch the server by `python -m torchctr.serve --name {name} --path path/to/model`\n",
    "    '''\n",
    "    import requests\n",
    "    import json\n",
    "\n",
    "    data = {\n",
    "        'features': df.to_dict(orient='list')\n",
    "    }\n",
    "    print(f\"Data: {data}\")\n",
    "    data_json = json.dumps(data)\n",
    "    response = requests.post(\n",
    "        f\"http://localhost:8000/{name}/predict\", \n",
    "        data=data_json, \n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        prediction = response.json()\n",
    "        print(f\"Prediction: {prediction}\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {'features': {'reviewerID': ['A2VXD6WHKCT5GC', 'A1TS6B7G1G5ESX', 'A11LO0DDZ9YT2A'], 'asin': ['B002RO8YK0', 'B0031RGEUE', 'B006QB1RPY'], 'unixReviewTime': [1317859200, 1382745600, 1361145600], 'overall': [5.0, 4.0, 5.0], 'title': ['50 Clear ClamShell CD DVD Case, Clam Shells', 'Olympus Stylus Tough 6020 14MP Digital Camera with 5x Wide Angle Zoom and 2.7 inch LCD (Green)', 'ASUS RT-N66U Dual-Band Wireless-N900 Gigabit Router'], 'price': [9.0, 188.99, 127.95], 'brand': ['mediaxpo', nan, 'Asus'], 'categories': ['Disc Jewel Cases', 'Cameras', 'Routers'], 'label': [1, 1, 1], 'his_asin_seq': [['B001NPEBGU', 'B0013EMKXC', 'B000R3CQM0', 'B0001FTVDQ'], ['B001V9KG0I'], ['B0013DZ9C2', 'B004V4N3NW', 'B004CQZVX4', 'B0040QE98O']]}}\n",
      "Prediction: [[1.8661800622940063], [1.6356334686279297], [3.7728238105773926]]\n"
     ]
    }
   ],
   "source": [
    "if ret == 0:\n",
    "    test_predict(df_samples.sample(3), name='dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
